{
  "hash": "452522b4465020df3f57cf6582659404",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 7\\n Time Series Linear Models\"\nformat: \n  revealjs:\n    output-file: \"7-tslm.html\"\n  html:\n    output-file: \"7-tslm_o.html\"\nlogo: \"../img/favicon.png\"\n---\n\n\n\n\n## Setup\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpp3)\n```\n:::\n\n\n\n\n\n## Multiple regression and forecasting {.smaller}\n\n\n  $$y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + \\cdots + \\beta_kx_{k,t} + \\varepsilon_t.$$\n\n\n* $y_t$ is the variable we want to predict: the \"response\" variable\n* Each $x_{j,t}$ is numerical and is called a \"predictor\".\n They are usually assumed to be known for all past and future times.\n* The coefficients $\\beta_1,\\dots,\\beta_k$ measure the effect of each\npredictor after taking account of the effect of all other predictors\nin the model.\n\n. . .\n\nThat is, the coefficients measure the \\orange{marginal effects}.\n\n* $\\varepsilon_t$ is a white noise error term\n\n\n## Example: US consumption expenditure\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_change |> autoplot(Consumption) +\n  geom_line(aes(y = Income, color = \"Income\")) +\n  geom_line(aes(y = Consumption, color = \"Consumption\")) +\n  scale_color_manual(values = c(\"Income\" = \"blue\", \"Consumption\" = \"red\")) +\n  labs(y = \"% change\", color = \"Legend\")\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/ConsInc-1.png){width=960}\n:::\n:::\n\n\n\n\n## Example: US consumption expenditure\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_change |> ggplot(aes(x=Income, y=Consumption)) +\n  labs(y = \"Consumption (quarterly % change)\",\n       x = \"Income (quarterly % change)\") +\n  geom_point() + geom_smooth(method=\"lm\", se=FALSE)\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/ConsInc2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Example: US consumption expenditure\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cons <- us_change |>\n  model(lm = TSLM(Consumption ~ Income))\nreport(fit_cons)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Consumption \nModel: TSLM \n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.58236 -0.27777  0.01862  0.32330  1.42229 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.54454    0.05403  10.079  < 2e-16 ***\nIncome       0.27183    0.04673   5.817  2.4e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5905 on 196 degrees of freedom\nMultiple R-squared: 0.1472,\tAdjusted R-squared: 0.1429\nF-statistic: 33.84 on 1 and 196 DF, p-value: 2.4022e-08\n```\n\n\n:::\n:::\n\n\n\n\n\n## Example: US consumption expenditure {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_change |>\n  gather(\"Measure\", \"Change\", Consumption, Income, Production, Savings, Unemployment) |>\n  ggplot(aes(x = Quarter, y = Change, colour = Measure)) +\n  geom_line() +\n  facet_grid(vars(Measure), scales = \"free_y\") +\n  labs(y = \"\") +\n  guides(colour = \"none\")\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/MultiPredictors-1.png){width=960}\n:::\n:::\n\n\n\n\n## Example: US consumption expenditure\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_change |>\n  as_tibble() |>\n  select(-Quarter) |>\n  GGally::ggpairs()\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/ScatterMatrix-1.png){width=960}\n:::\n:::\n\n\n\n\n##  Assumptions for the linear model\n\nFor forecasting purposes, we require the following assumptions:\n\n* $\\varepsilon_t$ have mean zero and are uncorrelated.\n\n* $\\varepsilon_t$ are uncorrelated with each $x_{j,t}$.\n\n. . .\n\nIt is *useful* to also have $\\varepsilon_t \\sim \\text{N}(0,\\sigma^2)$ when producing prediction intervals or doing statistical tests.\n\n## Least squares estimation {.smaller}\n\n* In practice we need to estimate the coefficients: $\\beta_0,\\beta_1, \\dots, \\beta_k$.\n\n. . .\n\n$$\\sum_{t=1}^T \\varepsilon_t^2 = \\sum_{t=1}^T (y_t -\n  \\beta_{0} - \\beta_{1} x_{1,t} - \\beta_{2} x_{2,t} - \\cdots - \\beta_{k} x_{k,t})^2$$ \n  \n. . .\n\n\n`model(TSLM(y ~ x_1 + x_2 + ... + x_k))`\n\n* Estimated coefficients: $\\hat\\beta_0, \\dots, \\hat\\beta_k$ \n\n\n## Example: US consumption expenditure\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_consMR <- us_change |>\n  model(lm = TSLM(Consumption ~ Income + Production + Unemployment + Savings))\nreport(fit_consMR)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Consumption \nModel: TSLM \n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90555 -0.15821 -0.03608  0.13618  1.15471 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.253105   0.034470   7.343 5.71e-12 ***\nIncome        0.740583   0.040115  18.461  < 2e-16 ***\nProduction    0.047173   0.023142   2.038   0.0429 *  \nUnemployment -0.174685   0.095511  -1.829   0.0689 .  \nSavings      -0.052890   0.002924 -18.088  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3102 on 193 degrees of freedom\nMultiple R-squared: 0.7683,\tAdjusted R-squared: 0.7635\nF-statistic:   160 on 4 and 193 DF, p-value: < 2.22e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Fitted values {.smaller}\n\n\n$$\\hat{y}_t = \\hat\\beta_{0} + \\hat\\beta_{1} x_{1,t} + \\hat\\beta_{2} x_{2,t} + \\cdots + \\hat\\beta_{k} x_{k,t}$$ \n\n\n:::{.panel-tabset}\n\n### Code\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(fit_consMR) |>\n  ggplot(aes(x = Quarter)) +\n  geom_line(aes(y = Consumption, colour = \"Data\")) +\n  geom_line(aes(y = .fitted, colour = \"Fitted\")) +\n  labs(y = NULL, title = \"Percent change in US consumption expenditure\") +\n  scale_colour_manual(values = c(Data = \"black\", Fitted = \"#D55E00\")) +\n  guides(colour = guide_legend(title = NULL))\n```\n:::\n\n\n\n\n\n### Plot\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-2-1.png){width=960 height=70%}\n:::\n:::\n\n\n\n\n:::\n\n## Example: US consumption expenditure\n\n:::{.panel-tabset}\n\n### Code\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(fit_consMR) |>\n  ggplot(aes(y = .fitted, x = Consumption)) +\n  geom_point() +\n  labs(\n    y = \"Fitted (predicted values)\",\n    x = \"Data (actual values)\",\n    title = \"Percentage change in US consumption expenditure\"\n  ) +\n  geom_abline(intercept = 0, slope = 1)\n```\n:::\n\n\n\n\n### Plot\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n\n## Goodness of fit {.smaller}\n\n**Coefficient of determination**\n\n$$R^2 = \\frac{\\sum(\\hat{y}_{t} - \\bar{y})^2}{\\sum(y_{t}-\\bar{y})^2}$$\n\n. . .\n\n**Standard error of the regression**\n\n  $$\\hat{\\sigma}_e=\\sqrt{\\frac{1}{T-k-1}\\sum_{t=1}^{T}{e_t^2}}$$\n\nwhere $k$ is the number of predictors in the model.\n\n \n\n## Multiple regression and forecasting {.smaller}\n\nFor forecasting purposes, we require the following assumptions:\n\n* $\\varepsilon_t$ are uncorrelated and zero mean\n\n* $\\varepsilon_t$ are uncorrelated with each $x_{j,t}$.\n\n. . .\n\nIt is **useful** to also have $\\varepsilon_t \\sim \\text{N}(0,\\sigma^2)$ when producing prediction intervals or doing statistical tests.\n\n## Residual plots {.smaller}\n\nUseful for spotting outliers and whether the linear model was appropriate.\n\n* Scatterplot of residuals $\\varepsilon_t$ against each predictor $x_{j,t}$.\n\n* Scatterplot residuals against the fitted values $\\hat y_t$\n\n* Expect to see scatterplots resembling a horizontal band with no values too far from the band and no patterns such as curvature or increasing spread.\n\n## Residual patterns {.smaller}\n\n* If a plot of the residuals vs any predictor in the model shows a pattern, then the relationship is nonlinear.\n\n* If a plot of the residuals vs any predictor **not** in the model shows a pattern, then the predictor should be added to the model.\n\n* If a plot of the residuals vs fitted values shows a pattern, then there is heteroscedasticity in the errors. (Could try a transformation.)\n\n\n## Trend\n\n**Linear trend**\n\n  $$x_t = t$$\n\n* $t=1,2,\\dots,T$\n* Strong assumption that trend will continue.\n\n## Dummy variables {.smaller}\n\n\nIf a categorical variable takes only two values (e.g., `Yes`, or `No`), then an equivalent numerical variable can be constructed taking value 1 if yes and 0 if no. This is called a *dummy variable*.\n\n\n| Variable | dummy |\n|----------|-------|\n| Yes      | 1     |\n| Yes      | 1     |\n| No       | 0     |\n| Yes      | 1     |\n| No       | 0     |\n| No       | 0     |\n| Yes      | 1     |\n| Yes      | 1     |\n| No       | 0     |\n| No       | 0     |\n\n\n\n## Dummy variables {.smaller}\n\nIf there are more than two categories, then the variable can be coded using several dummy variables (one fewer than the total number of categories).\n\n| Day       | d1 | d2 | d3 | d4 |\n|-----------|----|----|----|----|\n| Monday    | 1  | 0  | 0  | 0  |\n| Tuesday   | 0  | 1  | 0  | 0  |\n| Wednesday | 0  | 0  | 1  | 0  |\n| Thursday  | 0  | 0  | 0  | 1  |\n| Friday    | 0  | 0  | 0  | 0  |\n| Monday    | 1  | 0  | 0  | 0  |\n| Tuesday   | 0  | 1  | 0  | 0  |\n| Wednesday | 0  | 0  | 1  | 0  |\n| Thursday  | 0  | 0  | 0  | 1  |\n| Friday    | 0  | 0  | 0  | 0  |\n\n## Beware of the dummy variable trap! {.smaller}\n\n* Using one dummy for each category gives too many dummy variables!\n\n* The regression will then be singular and inestimable.\n\n* Either omit the constant, or omit the dummy for one category.\n\n* The coefficients of the dummies are relative to the omitted category.\n\n## Uses of dummy variables {.smaller}\n\n**Seasonal dummies**\n\n* For quarterly data: use 3 dummies\n* For monthly data: use 11 dummies\n* For daily data: use 6 dummies\n* What to do with weekly data?\n\n. . .\n\n**Outliers**\n\n* If there is an outlier, you can use a dummy variable to remove its effect.\n\n. . .\n\n**Public holidays**\n\n* For daily data: if it is a public holiday, dummy=1, otherwise dummy=0.\n\n## Beer production revisited (1/6) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecent_production <- aus_production |> filter(year(Quarter) >= 1992)\nrecent_production |> autoplot(Beer) +\n  labs(y = \"Megalitres\", title = \"Australian quarterly beer production\")\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/beer_2-1.png){width=960}\n:::\n:::\n\n\n\n\n. . .\n\n**Regression model**\n\n$$y_t = \\beta_0 + \\beta_1 t + \\beta_2d_{2,t} + \\beta_3 d_{3,t} + \\beta_4 d_{4,t} + \\varepsilon_t$$\n\n* $d_{i,t} = 1$ if $t$ is quarter $i$ and 0 otherwise.\n\n## Beer production revisited (2/6) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_beer <- recent_production |> model(TSLM(Beer ~ trend() + season()))\nreport(fit_beer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Beer \nModel: TSLM \n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-42.9029  -7.5995  -0.4594   7.9908  21.7895 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   441.80044    3.73353 118.333  < 2e-16 ***\ntrend()        -0.34027    0.06657  -5.111 2.73e-06 ***\nseason()year2 -34.65973    3.96832  -8.734 9.10e-13 ***\nseason()year3 -17.82164    4.02249  -4.430 3.45e-05 ***\nseason()year4  72.79641    4.02305  18.095  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.23 on 69 degrees of freedom\nMultiple R-squared: 0.9243,\tAdjusted R-squared: 0.9199\nF-statistic: 210.7 on 4 and 69 DF, p-value: < 2.22e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Beer production revisited (3/6) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(fit_beer) |>\n  ggplot(aes(x = Quarter)) +\n  geom_line(aes(y = Beer, colour = \"Data\")) +\n  geom_line(aes(y = .fitted, colour = \"Fitted\")) +\n  labs(y = \"Megalitres\", title = \"Australian quarterly beer production\") +\n  scale_colour_manual(values = c(Data = \"black\", Fitted = \"#D55E00\"))\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n\n## Beer production revisited (4/6) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(fit_beer) |>\n  ggplot(aes(x = Beer, y = .fitted, colour = factor(quarter(Quarter)))) +\n  geom_point() +\n  labs(y = \"Fitted\", x = \"Actual values\", title = \"Quarterly beer production\") +\n  scale_colour_brewer(palette = \"Dark2\", name = \"Quarter\") +\n  geom_abline(intercept = 0, slope = 1)\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n\n## Beer production revisited (5/6) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_beer |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n\n## Beer production revisited (6/6) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_beer |>\n  forecast() |>\n  autoplot(recent_production)\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n\n## Intervention variables {.smaller}\n\n**Spikes**\n\n* Equivalent to a dummy variable for handling an outlier.\n\n. . .\n\n**Steps**\n\n* Variable takes value 0 before the intervention and 1 afterwards.\n\n. . .\n\n**Change of slope**\n\n* Variables take values 0 before the intervention and values $\\{1,2,3,\\dots\\}$ afterwards.\n\n## Holidays {.smaller}\n\n**For monthly data**\n\n* Christmas: always in December so part of monthly seasonal effect\n* Easter: use a dummy variable $v_t=1$ if any part of Easter is in that month, $v_t=0$ otherwise.\n* Ramadan and Chinese new year similar.\n\n## Distributed lags {.smaller}\n\nLagged values of a predictor.\n\nExample: $x$ is advertising which has a delayed effect\n\n\\begin{align*}\n  x_{1} &= \\text{advertising for previous month;} \\\\\n  x_{2} &= \\text{advertising for two months previously;} \\\\\n        & \\vdots \\\\\n  x_{m} &= \\text{advertising for $m$ months previously.}\n\\end{align*}\n\n## Fourier series {.smaller}\n\nPeriodic seasonality can be handled using pairs of Fourier terms:\n\n$$s_{k}(t) = \\sin\\left(\\frac{2\\pi k t}{m}\\right)\\qquad c_{k}(t) = \\cos\\left(\\frac{2\\pi k t}{m}\\right)$$\n\n$$y_t = a + bt + \\sum_{k=1}^K \\left[\\alpha_k s_k(t) + \\beta_k c_k(t)\\right] + \\varepsilon_t$$\nwhere $m$ is the seasonal period. \n\n* Every periodic function can be approximated by sums of sin and cos terms for large enough $K$.\n* Choose $K$ by minimizing AICc.\n* Called \"harmonic regression\"\n\n. . .\n\n`TSLM(y ~ trend() + fourier(K))`\n\n## Harmonic regression: beer production\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfourier_beer <- recent_production |> model(TSLM(Beer ~ trend() + fourier(K = 2)))\nreport(fourier_beer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Beer \nModel: TSLM \n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-42.9029  -7.5995  -0.4594   7.9908  21.7895 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        446.87920    2.87321 155.533  < 2e-16 ***\ntrend()             -0.34027    0.06657  -5.111 2.73e-06 ***\nfourier(K = 2)C1_4   8.91082    2.01125   4.430 3.45e-05 ***\nfourier(K = 2)S1_4 -53.72807    2.01125 -26.714  < 2e-16 ***\nfourier(K = 2)C2_4 -13.98958    1.42256  -9.834 9.26e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.23 on 69 degrees of freedom\nMultiple R-squared: 0.9243,\tAdjusted R-squared: 0.9199\nF-statistic: 210.7 on 4 and 69 DF, p-value: < 2.22e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_cafe <- aus_retail |>\n  filter(Industry == \"Cafes, restaurants and takeaway food services\",\n         year(Month) %in% 2004:2018) |>\n  summarise(Turnover = sum(Turnover))\naus_cafe |> autoplot(Turnover)\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe_-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure (1/7) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aus_cafe |>\n  model(\n    K1 = TSLM(log(Turnover) ~ trend() + fourier(K = 1)),\n    K2 = TSLM(log(Turnover) ~ trend() + fourier(K = 2)),\n    K3 = TSLM(log(Turnover) ~ trend() + fourier(K = 3)),\n    K4 = TSLM(log(Turnover) ~ trend() + fourier(K = 4)),\n    K5 = TSLM(log(Turnover) ~ trend() + fourier(K = 5)),\n    K6 = TSLM(log(Turnover) ~ trend() + fourier(K = 6))\n  )\n```\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure (2/7) {.smaller}\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe1-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure (3/7) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure (4/7) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe3-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure (5/7) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe4-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure (6/7) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe5-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure (7/7) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe6-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Fourier series {.smaller}\n\nPeriodic seasonality can be handled using pairs of Fourier terms:\n$$s_{k}(t) = \\sin\\left(\\frac{2\\pi k t}{m}\\right)\\qquad c_{k}(t) = \\cos\\left(\\frac{2\\pi k t}{m}\\right)$$\n$$y_t = a + bt + \\sum_{k=1}^K \\left[\\alpha_k s_k(t) + \\beta_k c_k(t)\\right] + \\varepsilon_t$$\n\n* Every periodic function can be approximated by sums of sin and cos terms for large enough $K$.\n* $K \\le m/2$\n* $m$ can be non-integer\n* Particularly useful for large $m$.\n\n## Comparing regression models {.smaller}\n\nComputer output for regression will always give the $R^2$ value. This is a useful summary of the model.\n\n* It is equal to the square of the correlation between $y$ and $\\hat y$.\n* It is often called the \"coefficient of determination\".\n* It can also be calculated as follows:\n\n. . .\n\n$$R^2 = \\frac{\\sum(\\hat{y}_t - \\bar{y})^2}{\\sum(y_t-\\bar{y})^2}$$\n\n\n* It is the proportion of variance accounted for (explained) by the predictors.\n\n## Comparing regression models {.smaller}\n\n\nHowever ...\n\n* $R^2$  does not allow for \"degrees of freedom\".\n\n* Adding *any* variable tends to increase the value of $R^2$, even if that variable is irrelevant.\n\n. . .\n\nTo overcome this problem, we can use *adjusted $R^2$*:\n\n$$\\bar{R}^2 = 1-(1-R^2)\\frac{T-1}{T-k-1}$$\n\nwhere $k=$ no. predictors and $T=$ no. observations.\n\n\n. . .\n\n:::{.callout-note}\n\nMaximizing $\\bar{R}^2$ is equivalent to minimizing $\\hat\\sigma^2$.\n\n$$\\hat{\\sigma}^2 = \\frac{1}{T-k-1}\\sum_{t=1}^T \\varepsilon_t^2$$\n\n:::\n\n## Akaike's Information Criterion {.smaller}\n\n\n$$\\text{AIC} = -2\\log(L) + 2(k+2)$$\n\nwhere $L$ is the likelihood and $k$ is the number of predictors in the model.\n\n* AIC penalizes terms more heavily than $\\bar{R}^2$.\n* Minimizing the AIC is asymptotically equivalent to minimizing MSE via **leave-one-out cross-validation** (for any linear regression).\n\n## Corrected AIC {.smaller}\n\nFor small values of $T$, the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed.\n\n\n$$\\text{AIC}_{\\text{C}} = \\text{AIC} + \\frac{2(k+2)(k+3)}{T-k-3}$$\n\n. . .\n\nAs with the AIC, the AIC$_{\\text{C}}$ should be minimized.\n\n## Bayesian Information Criterion {.smaller}\n\n\n$$\\text{BIC} = -2\\log(L) + (k+2)\\log(T)$$\n\n\nwhere $L$ is the likelihood and $k$ is the number of predictors in the model.\n\n* BIC penalizes terms more heavily than AIC\n\n* Also called SBIC and SC.\n\n* Minimizing BIC is asymptotically equivalent to leave-$v$-out cross-validation when $v = T[1-1/(log(T)-1)]$.\n\n\n## Harmonic regression: eating-out expenditure again (1/8) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_cafe <- aus_retail |>\n  filter(Industry == \"Cafes, restaurants and takeaway food services\",\n         year(Month) %in% 2004:2018) |>\n  summarise(Turnover = sum(Turnover))\naus_cafe |> autoplot(Turnover)\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure again (2/8) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aus_cafe |>\n  model(\n    K1 = TSLM(log(Turnover) ~ trend() + fourier(K = 1)),\n    K2 = TSLM(log(Turnover) ~ trend() + fourier(K = 2)),\n    K3 = TSLM(log(Turnover) ~ trend() + fourier(K = 3)),\n    K4 = TSLM(log(Turnover) ~ trend() + fourier(K = 4)),\n    K5 = TSLM(log(Turnover) ~ trend() + fourier(K = 5)),\n    K6 = TSLM(log(Turnover) ~ trend() + fourier(K = 6))\n  )\nglance(fit) |> select(.model, r_squared, adj_r_squared, CV, AICc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  .model r_squared adj_r_squared      CV   AICc\n  <chr>      <dbl>         <dbl>   <dbl>  <dbl>\n1 K1         0.962         0.962 0.00238 -1085.\n2 K2         0.966         0.965 0.00220 -1099.\n3 K3         0.976         0.975 0.00157 -1160.\n4 K4         0.980         0.979 0.00138 -1183.\n5 K5         0.985         0.984 0.00104 -1234.\n6 K6         0.985         0.984 0.00105 -1232.\n```\n\n\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure again (3/8) {.smaller}\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe1_2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure again (4/8) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe2_2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure again (5/8) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe3_2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure again (6/8) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe4_2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure again (7/8) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe5_2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Harmonic regression: eating-out expenditure again (8/8) {.smaller}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/cafe6_2-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Ex-ante versus ex-post forecasts {.smaller}\n\n * **Ex ante forecasts** are made using only information available in advance.\n    - require forecasts of predictors\n * **Ex post forecasts** are made using later information on the predictors.\n    - useful for studying behavior of forecasting models.\n\n * trend, seasonal and calendar variables are all known in advance, so these don't need to be forecast.\n\n## Beer production\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecent_production <- aus_production |> filter(year(Quarter) >= 1992)\nrecent_production |> model(TSLM(Beer ~ trend() + season())) |> \n   forecast() |> autoplot(recent_production)\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/beeryetagain-1.png){width=960}\n:::\n:::\n\n\n\n\n## Scenario based forecasting\n\n * Assumes possible scenarios for the predictor variables\n * Prediction intervals for scenario based forecasts do not include the uncertainty associated with the future values of the predictor variables.\n\n\n## US Consumption\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_consBest <- us_change |>\n  model(\n    TSLM(Consumption ~ Income + Savings + Unemployment)\n  )\n\nfuture_scenarios <- scenarios(\n  Increase = new_data(us_change, 4) |>\n    mutate(Income = 1, Savings = 0.5, Unemployment = 0),\n  Decrease = new_data(us_change, 4) |>\n    mutate(Income = -1, Savings = -0.5, Unemployment = 0),\n  names_to = \"Scenario\"\n)\n\nfc <- forecast(fit_consBest, new_data = future_scenarios)\n```\n:::\n\n\n\n\n## US Consumption\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_change |> autoplot(Consumption) +\n  labs(y = \"% change in US consumption\") +\n  autolayer(fc) +\n  labs(title = \"US consumption\", y = \"% change\")\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/usconsumptionf2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Building a predictive regression model \n\n * If getting forecasts of predictors is difficult, you can use lagged predictors instead.\n \n. . .\n\n$$y_{t+h}=\\beta_0+\\beta_1x_{1,t}+\\dots+\\beta_kx_{k,t}+\\varepsilon_{t+h}$$\n\n * A different model for each forecast horizon $h$.\n\n\n## Nonlinear regression {.smaller}\n\nA **log-log** functional form \n\n$$\\log y=\\beta_0+\\beta_1 \\log x +\\varepsilon$$\n\nwhere $\\beta_1$ is interpreted as an elasticity (the average percentage change in $y$ resulting from a $1\\%$ increase in $x$). \n\n- alternative specifications: log-linear, linear-log.\n- use $\\log(x+1)$ if required.\n \n\n## Piecewise linear and regression splines (1/2) {.smaller}\n\n$$y=f(x) +\\varepsilon$$ where $f$ is a non-linear function. \n\n- For **piecewise linear** let $x_1=x$ and \n\n. . .\n\n\\begin{align*}\n  x_{2} = (x-c)_+ &= \\left\\{\n             \\begin{array}{ll}\n               0 & \\text{if } x < c\\\\\n               x-c &  \\text{if } x \\ge c.\n             \\end{array}\\right.\n\\end{align*} \n\n\n## Piecewise linear and regression splines (2/2) {.smaller}\n\n- In general, **Linear Regression Splines**\n\n. . .\n\n$$x_1=x~~~x_2=(x-c_1)_+~~~\\ldots~~~x_k=(x-c_{k-1})_+$$  \n\nwhere $c_1,\\ldots,c_{k-1}$ are knots.\n\n* Need to select knots: can be difficult and arbitrary. \n* Automatic knot selection algorithms very slow.\n* Using piecewise cubics achieves a smoother result.\n\n. . .\n\n:::{.callout-warning}\nBetter fit but forecasting outside the range of the historical data is even more unreliable.\n:::\n\n## Nonlinear trends {.smaller}\n\nPiecewise linear trend with bend at $\\tau$\n\n. . .\n\n\\begin{align*}\nx_{1,t} &= t \\\\\nx_{2,t} &= \\left\\{ \\begin{array}{ll}\n  0 & t <\\tau\\\\\n  (t-\\tau) & t \\ge \\tau\n\\end{array}\\right.\n\\end{align*}\n\n. . .\n\nQuadratic or higher order trend\n\n\n$$x_{1,t} =t,\\quad x_{2,t}=t^2,\\quad \\dots$$\n\n. . .\n\n:::{.callout-warning}\nNOT RECOMMENDED\n:::\n\n\n## Example: Boston marathon winning times (1/4) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmarathon <- boston_marathon |>\n  filter(Event == \"Men's open division\") |>\n  select(-Event) |>\n  mutate(Minutes = as.numeric(Time) / 60)\nmarathon |> autoplot(Minutes) + labs(y = \"Winning times in minutes\")\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n## Example: Boston marathon winning times (2/4) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_trends <- marathon |>\n  model(\n    # Linear trend\n    linear = TSLM(Minutes ~ trend()),\n    # Exponential trend\n    exponential = TSLM(log(Minutes) ~ trend()),\n    # Piecewise linear trend\n    piecewise = TSLM(Minutes ~ trend(knots = c(1940, 1980)))\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_trends\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 1 x 3\n   linear exponential piecewise\n  <model>     <model>   <model>\n1  <TSLM>      <TSLM>    <TSLM>\n```\n\n\n:::\n:::\n\n\n\n\n## Example: Boston marathon winning times (3/4) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_trends |>\n  forecast(h = 10) |>\n  autoplot(marathon)\n```\n:::\n\n\n\n\n:::{.panel-tabset}\n\n### Code\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc_trends <- fit_trends |> forecast(h = 10)\nmarathon |>\n  autoplot(Minutes) +\n  geom_line(\n    data = fitted(fit_trends),\n    aes(y = .fitted, colour = .model)\n  ) +\n  autolayer(fc_trends, alpha = 0.5, level = 95) +\n  labs(\n    y = \"Minutes\",\n    title = \"Boston marathon winning times\"\n  )\n```\n:::\n\n\n\n\n### Plot\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n\n## Example: Boston marathon winning times (4/4) {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_trends |>\n  select(piecewise) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](7-tslm_files/figure-revealjs/residPiecewise-1.png){width=960}\n:::\n:::\n\n\n\n\n## Correlation is not causation {.smaller}\n\n* When $x$ is useful for predicting $y$, it is not necessarily causing $y$.\n\n* e.g., predict number of swimmers $y$ using number of ice-creams sold $x$.\n\n* Correlations are useful for forecasting, even when there is no causality.\n\n* Better models usually involve causal relationships (e.g., temperature $x$ and people $z$ to predict swimmers $y$).\n\n## Multicollinearity {.smaller}\n\nIn regression analysis, multicollinearity occurs when:\n\n*  Two  predictors are highly  correlated (i.e., the correlation between them is close to $\\pm1$).\n* A linear combination of some of the predictors is highly correlated  with another predictor.\n*  A linear combination of one subset of predictors is highly correlated with a linear combination of another\n  subset of predictors.\n\n## Multicollinearity {.smaller}\n\nIf multicollinearity exists\\dots\n\n* the numerical estimates of coefficients may be wrong (worse in Excel than in a statistics package)\n* don't rely on the $p$-values to determine significance.\n* there is no problem with model *predictions* provided the predictors used for forecasting are within the range used for fitting.\n* omitting variables can help.\n* combining variables can help.\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}