---
title: "Chapter 3\nDecomposition"
format: 
  revealjs:
    output-file: "3-decomp.html"
  html:
    output-file: "3-decomp_o.html"
logo: "../img/favicon.png"
---

## Setup

```{r setup}
library(purrr)
library(gganimate)
library(latex2exp)
library(fpp3)
```

# Transformations and adjustments

## Per capita adjustments

```{r gdp-per-capita}
global_economy |>
  filter(Country == "Australia") |>
  autoplot(GDP)
```

## Per capita adjustments

```{r gdp-per-capita2}
global_economy |>
  filter(Country == "Australia") |>
  autoplot(GDP / Population)
```

## You Try!

:::{.question}
Consider the GDP information in `global_economy`. Plot the GDP per capita for each country over time. Which country has the highest GDP per capita? How has this changed over time? -->
:::

## Inflation adjustments

- Data which are affected by the value of money are best adjusted before modelling. 
 - For example, the average cost of a new house will have increased over the last few decades due to inflation. A \$200,000 house this year is not the same as a \$200,000 house twenty years ago. 

## Inflation adjustments {.smaller}

- To make these adjustments, a price index is used. If $z_t$ denotes the price index and $y_t$ denotes the original house price in year $t$, then 

. . .

$$x_t=y_t/z_t \cdot z_{2000}$$

gives the adjusted house price at year 2000 dollar values. 

- Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).

## Inflation adjustments

:::{.panel-tabset}

### Code 

```{r retail_cpi, message=FALSE, warning=FALSE, fig.show='hide'}
aus_newspaper_retail <- readr::read_csv('data/aus_newspaper_retail.csv')
# Turnover:	Retail turnover in $Million AUD

aus_newspaper_retail <- aus_newspaper_retail |>
  select(Year,Turnover,name) |> # Picks out these 3 columns/variables
  group_by(Year,name) |> # Tells are to now calculate by groups by year and name
  summarise(sum_Turnover = sum(Turnover))|> #adds up Turnover for each year and name combination
  as_tsibble(index = Year, key = "name") # Converts to time series

aus_newspaper_retail |> autoplot(sum_Turnover) +
  facet_grid(name ~ ., scales = "free_y") + # This will make a grid of plots
    # with name used to break into multiple plots along the y direction
  labs(title = "Turnover: Australian print media industry", y = "$AU")

```



### Plot

```{r ref.label = 'retail_cpi', message=FALSE, warning=FALSE, echo=FALSE}
```


:::

## Mathematical transformations {.smaller}


If the data show different variation at different levels of the series, then a transformation can be useful.

. . .

Denote original observations as $y_1,\dots,y_T$ and transformed observations as $w_1, \dots, w_T$.

. . .

\underline{Mathematical transformations for stabilizing}

| Function      |                         | Impact        |
|---------------|-------------------------|---------------|
| Square root 	| $w_t = \sqrt{y_t}$    	| $\downarrow$ 	|
| Cube root   	| $w_t = \sqrt[3]{y_t}$ 	| Increasing   	|
| Logarithm   	| $w_t = \log(y_t)$     	| strength     	|


. . .

Logarithms, in particular, are useful because they are more interpretable: changes in a log value are **relative (percent) changes on the original scale**.

## Mathematical transformations (1/4)

```{r food}
food <- aus_retail |>
  filter(Industry == "Food retailing") |>
  summarise(Turnover = sum(Turnover))
```

```{r food-plot, echo = FALSE}
food |> autoplot(Turnover) +
  labs(y = "Turnover ($AUD)")
```

## Mathematical transformations (2/4)

```{r food-sqrt1}
food |> autoplot(sqrt(Turnover)) +
  labs(y = "Square root turnover")
```

## Mathematical transformations (3/4)

```{r food-cbrt}
food |> autoplot(Turnover^(1/3)) +
  labs(y = "Cube root turnover")
```

## Mathematical transformations (4/4)

```{r food-log}
food |> autoplot(log(Turnover)) +
  labs(y = "Log turnover")
```

. . .

(log here is the natural log, base $e$)

## Box-Cox transformations {.smaller}

Each of these transformations is close to a member of the
family of \textbf{Box-Cox transformations}:

$$w_t = \left\{\begin{array}{ll}
        \log(y_t),      & \quad \lambda = 0; \\
        (sign(y_t)|y_t|^\lambda-1)/\lambda ,         & \quad \lambda \ne 0.
\end{array}\right.$$

- Actually the Bickel-Doksum transformation (allowing for $y_t<0$)
- $\lambda=1$: (No substantive transformation)
- $\lambda=\frac12$: (Square root plus linear transformation)
- $\lambda=0$: (Natural logarithm)

## Box-Cox transformations

```{r food-anim, cache=TRUE, echo=FALSE, fig.show='animate', interval=1/10, message=FALSE, fig.height=4.5, fig.width=9, aniopts='controls,buttonsize=0.3cm,width=13.5cm'}

food |>
  mutate(!!!set_names(map(seq(0, 1, 0.01), ~ expr(fabletools::box_cox(Turnover, !!.x))), seq(0, 1, 0.01))) |>
  select(-Turnover) |>
  pivot_longer(-Month, names_to = "lambda", values_to = "Turnover") |>
  mutate(lambda = as.numeric(lambda)) |>
  ggplot(aes(x = Month, y = Turnover)) +
  geom_line() +
  transition_states(1 - lambda, state_length = 0) +
  view_follow() +
  labs(title = "Box-Cox transformed food retailing turnover (lambda = {format(1 - as.numeric(closest_state), digits = 2)})")
```

## Box-Cox transformations {.smaller}

```{r food-lambda}
food |>
  features(Turnover, features = guerrero)
```

- This attempts to balance the seasonal fluctuations and random variation across the series.
- Always check the results.
- A low value of $\lambda$ can give extremely large prediction intervals.

## Box-Cox transformations

```{r food-bc}
food |> autoplot(box_cox(Turnover, 0.0524)) +
  labs(y = "Box-Cox transformed turnover")
```

## Transformations {.smaller}



- Often no transformation needed.
- Simple transformations are easier to explain and work well enough.
- Transformations can have very large effect on PI.
- If some data are zero or negative, then use $\lambda>0$.
- `log1p()` can also be useful for data with zeros.
- Choosing logs is a simple way to force forecasts to be positive
- Transformations must be reversed to obtain forecasts on the original scale. (Handled automatically by `fable`.)


## You Try! {.smaller}

1. For the following series, find an appropriate transformation in order to stabilise the variance.

    - United States GDP from `global_economy`
    - Slaughter of Victorian “Bulls, bullocks and steers” in `aus_livestock`
    - Victorian Electricity Demand from `vic_elec`.
    - Gas production from `aus_production`

2. Why is a Box-Cox transformation unhelpful for the `canadian_gas` data?




# Time series components

## Time series patterns {.smaller}

**Recall**

Trend
:  pattern exists when there is a long-term increase or decrease in the data.

Cyclic
: pattern exists when data exhibit rises and falls that are *not of fixed period* (duration usually of at least 2 years).

Seasonal
: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).

## Time series decomposition {.smaller}

$y_t = f(S_t, T_t, R_t)$

where $y_t=$, data at period $t$ 

$T_t=$, trend-cycle component at period $t$

$S_t=$, seasonal component at period $t$ 

$R_t=$,remainder component at period $t$

. . .

**Additive decomposition:** $y_t = S_t + T_t + R_t.$

. . .

**Multiplicative decomposition:** $y_t = S_t \times T_t \times R_t.$

## Time series decomposition {.smaller}

  -  Additive model  appropriate if  magnitude of  seasonal fluctuations does not vary with level.
  -  If seasonal are proportional to level of series, then multiplicative model appropriate.
  -  Multiplicative decomposition more prevalent with economic series
  -  Alternative: use a Box-Cox transformation, and then use additive decomposition.
  -  Logs turn multiplicative relationship into an additive relationship:

. . .

$$y_t = S_t \times T_t \times R_t \quad\Rightarrow\quad
\log y_t = \log S_t + \log T_t + \log R_t.$$

## US Retail Employment

```{r usretail}
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)
us_retail_employment
```



## US Retail Employment

```{r dable1}
us_retail_employment |>
  autoplot(Employed) +
  labs(y="Persons (thousands)", title="Total employment in US retail")
```


## US Retail Employment

```{r dable2}
us_retail_employment |>
  model(stl = STL(Employed))
```


## US Retail Employment



```{r dable3}
dcmp <- us_retail_employment |>
  model(stl = STL(Employed))
components(dcmp)
```


## US Retail Employment

```{r dable4}
us_retail_employment |>
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), trend, color='#523178') +
  labs(y="Persons (thousands)", title="Total employment in US retail")
```


## US Retail Employment

```{r usretail-stl}
components(dcmp) |> autoplot()
```

## US Retail Employment

```{r usretail3}
components(dcmp) |> gg_subseries(season_year)
```

## Seasonal adjustment

  -  Useful by-product of decomposition:  an easy way to calculate seasonally adjusted data.
  -  Additive decomposition: seasonally adjusted data given by
$$y_t - S_t = T_t + R_t$$
  -  Multiplicative decomposition: seasonally adjusted data given by
$$y_t / S_t = T_t \times R_t$$

## US Retail Employment

```{r usretail-sa}
us_retail_employment |>
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), season_adjust, color='#0072B2') +
  labs(y="Persons (thousands)", title="Total employment in US retail")
```

## Seasonal adjustment

  - We use estimates of $S$ based on past values to seasonally adjust a current value.
  -  Seasonally adjusted series reflect **remainders** as well as **trend**. Therefore they are not "smooth" and "downturns" or "upturns" can be misleading.
  -  It is better to use the trend-cycle component to look for turning points.

# Moving Averages (MA)

## Moving Averages {.smaller}

- The classical method of time series decomposition originated in the 1920s and was widely used until the 1950s. 
- A moving average of order $m$ can be written as 
$$\hat{T}_t = \frac{1}{m}\Sigma_{j=-k}^k y_{t+j}$$
where $m = 2k+1$. 
- That is, the estimate of the trend-cycle at time $t$ is obtained by averaging values of the time series within $k$ periods of $t$.


## MA

- Averaging eliminates some of the randomness in the data, leaving a smooth trend-cycle component. 
- Called an m-MA, or MA(m), meaning a moving average of order m.

## MA global economy

```{r globa_econ_MA}
global_economy |>
  filter(Country == "Australia") |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")
```

## MA global economy

- A moving average is a an average that moves...
- MA(5) would mean we use the average of two time series values before $y_t$, $y_t$ itself, and the two values after $y_t$.
 - Ex: to get the MA(5) for $y_{100}$ we would use the average of $y_{98},y_{99},y_{100},y_{101}$ and $y_{102}$. 
- What is a limitation of this method? 

## MA global economy sliding window

- slide_dbl() from the slider package applies a function to “sliding” time windows. In this case, we use the mean() function with a window of size 5.

. . .

```{r globa_econ_sliding_window}
aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE)
  )
```

## MA trend line

:::{.panel-tabset}

### Code

```{r globa_econ_MA_line, warning=FALSE, fig.show='hide'}
aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports") 
```

### Plot
```{r ref.label='globa_econ_MA_line', message=FALSE, warning=FALSE, echo=FALSE}
```

:::

# History of time series decomposition

## History of time series decomposition

  -  Classical method originated in 1920s.
  -  Census II method introduced in 1957. Basis for X-11 method and variants (including X-12-ARIMA, X-13-ARIMA)
  -  STL method introduced in 1983
  -  TRAMO/SEATS introduced in 1990s.



## National Statistics Offices
 - ABS uses X-12-ARIMA
 - US Census Bureau uses X-13ARIMA-SEATS
 - Statistics Canada uses X-12-ARIMA
 - ONS (UK) uses X-12-ARIMA
 - EuroStat use X-13ARIMA-SEATS

## X-11 decomposition {.smaller}

**Advantages**

  -  Relatively robust to outliers
  -  Completely automated choices for trend and seasonal changes
  -  Very widely tested on economic data over a long period of time.

. . .

**Disadvantages**

  -  No prediction/confidence intervals
  -  Ad hoc method with no underlying model
  -  Only developed for quarterly and monthly data

## Extensions: X-12-ARIMA and X-13-ARIMA {.smaller}

  -  The X-11, X-12-ARIMA and X-13-ARIMA methods are based on Census II decomposition.
  -  These allow adjustments for trading days and other explanatory variables.
  -  Known outliers can be omitted.
  -  Level shifts and ramp effects can be modelled.
  -  Missing values estimated and replaced.
  -  Holiday factors (e.g., Easter, Labour Day) can be estimated.

## X-13ARIMA-SEATS {.smaller}

**Advantages**

  - Model-based
  - Smooth trend estimate
  - Allows estimates at end points
  - Allows changing seasonality
  - Developed for economic data

. . .

**Disadvantages**

  -  Only developed for quarterly and monthly data

# STL decomposition

## STL decomposition {.smaller}

  -  STL: "Seasonal and Trend decomposition using Loess"
  -  Very versatile and robust.
  -  Unlike X-12-ARIMA, STL will handle any type of seasonality.
  -  Seasonal component allowed to change over time, and rate of change controlled by user.
  -  Smoothness of trend-cycle also controlled by user.
  -  Robust to outliers
  -  Not trading day or calendar adjustments.
  -  Only additive.
  -  Take logs to get multiplicative decomposition.
  -  Use Box-Cox transformations to get other decompositions.

## STL decomposition

```{r stlwindow9, warning=FALSE, fig.width=8.5, fig.height=3.4}
us_retail_employment |>
  model(STL(Employed ~ season(window=9), robust=TRUE)) |>
  components() |> autoplot() +
    labs(title = "STL decomposition: US retail employment")
```

## STL decomposition

```{r stlwindowanim, echo=FALSE, warning=FALSE, message=FALSE, fig.show='animate', interval=1/10,  fig.height=5.35, fig.width=8, aniopts='controls,buttonsize=0.3cm,width=11.5cm', eval=TRUE}
s_windows <- seq(5,55,by=2)
stl_defs <- purrr::map(s_windows, function(s_window){
  STL(Employed ~ season(window=s_window), robust=TRUE)
})
names(stl_defs) <- sprintf("season(window=%02d)", s_windows)

us_retail_employment |>
  model(!!!stl_defs) |>
  components() |>
  as_tibble() |>
  pivot_longer(Employed:remainder,
               names_to = "component", names_ptypes = list(component = factor(levels = c("Employed", "trend", "season_year", "remainder"))),
               values_to = "Employed") |>
  ggplot(aes(x = Month, y = Employed)) +
  geom_line() +
  facet_grid(rows = vars(component), scales = "free_y") +
  labs(title = "STL decomposition of US retail employment",
       subtitle = "{closest_state}") +
  transition_states(.model)
```


## STL decomposition {.smaller}

```{r echo = TRUE, results = 'hide'}
us_retail_employment |>
  model(STL(Employed ~ season(window=5))) |>
  components()

us_retail_employment |>
  model(STL(Employed ~ trend(window=15) +
                       season(window="periodic"),
            robust = TRUE)
  ) |> components()
```


  -  `trend(window = ?)` controls wiggliness of trend component.
  -  `season(window = ?)` controls variation on seasonal component.
  -  `season(window = 'periodic')` is equivalent to an infinite window.

## STL decomposition {.smaller}

```{r mstl, fig.width=8.5, fig.height=3.4}
us_retail_employment |>
  model(STL(Employed)) |>
  components() |> autoplot()
```

:::{.callout-note}
`STL()` chooses \texttt{season(window=13)} by default. This can include transformations.
:::

## STL decomposition {.smaller}

- Algorithm that updates trend and seasonal components iteratively.
- Starts with $\hat{T}_t=0$
- Uses a mixture of loess and moving averages to successively refine the trend and seasonal estimates.
- The trend window controls loess bandwidth applied to deasonalised values.
- The season window controls loess bandwidth applied to detrended subseries.
- Robustness weights based on remainder.
- Default season `window = 13`
- Default trend `window = nextodd(` \newline\mbox{}\hfill `ceiling((1.5*period)/(1-(1.5/s.window)))`
