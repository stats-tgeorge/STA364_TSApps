---
title: "Chapter 9\nARIMA"
format: 
  revealjs:
    output-file: "9-arima.html"
  html:
    output-file: "9-arima_o.html"
logo: "../img/favicon.png"
---


## Setup

```{r setup, include=T}
library(patchwork)
library(purrr)
library(fpp3) # Primary
```

## ARIMA models {.smaller}

| **Component** | **Description**                                      |
|---------------|------------------------------------------------------|
| **AR**        | autoregressive (lagged observations as inputs)       |
| **I**         | integrated (differencing to make series stationary)  |
| **MA**        | moving average (lagged errors as inputs)             |

. . .


**An ARIMA model is rarely interpretable in terms of visible data structures like trend and seasonality. But it can capture a huge range of time series patterns.**

# Stationarity and differencing

## Stationarity

**Definition**
If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.


. . .

A **stationary series** is:

* roughly horizontal
* constant variance
* no patterns predictable in the long-term

## Stationary? (1/9)

```{r}
gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2018) |>
  autoplot(Close) +
  labs(y = "Google closing stock price", x = "Day")
```

## Stationary? (2/9)

```{r}
gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2018) |>
  autoplot(difference(Close)) +
  labs(y = "Google closing stock price", x = "Day")
```

## Stationary? (3/9)

```{r}
global_economy |>
  filter(Country == "Algeria") |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Algerian Exports")
```

## Stationary? (4/9)

```{r}
aus_production |>
  autoplot(Bricks) +
  labs(title = "Clay brick production in Australia")
```

## Stationary? (5/9)

```{r}
prices |>
  filter(year >= 1900) |>
  autoplot(eggs) +
  labs(y="$US (1993)", title="Price of a dozen eggs")
```

## Stationary? (6/9)

```{r}
aus_livestock |>
  filter(Animal == "Pigs", State == "Victoria") |>
  autoplot(Count/1e3) +
  labs(y = "thousands", title = "Total pigs slaughtered in Victoria")
```

## Stationary? (7/9)

```{r}
aus_livestock |>
  filter(Animal == "Pigs", State == "Victoria", year(Month) >= 2010) |>
  autoplot(Count/1e3) +
  labs(y = "thousands", title = "Total pigs slaughtered in Victoria")
```

## Stationary? (8/9)

```{r}
aus_livestock |>
  filter(Animal == "Pigs", State == "Victoria", year(Month) >= 2015) |>
  autoplot(Count/1e3) +
  labs(y = "thousands", title = "Total pigs slaughtered in Victoria")
```

## Stationary? (9/9)

```{r}
pelt |>
  autoplot(Lynx) +
  labs(y = "Number trapped", title = "Annual Canadian Lynx Trappings")
```

## Stationarity

**Definition**

If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.


- Transformations help to **stabilize the variance**.

- For ARIMA modelling, we also need to **stabilize the mean**.

## Non-stationarity in the mean

**Identifying non-stationary series**

* time plot.
* The ACF of stationary data drops to zero relatively quickly
* The ACF of non-stationary data decreases slowly.
* For non-stationary data, the value of $r_1$ is often large and positive.

## Example: Google stock price (1/5)

```{r}
google_2018 <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2018)
```

## Example: Google stock price (2/5)

```{r}
google_2018 |>
  autoplot(Close) +
  labs(y = "Closing stock price ($USD)")
```

## Example: Google stock price (3/5)

```{r, fig.height=3.5}
google_2018 |> ACF(Close) |> autoplot()
```

## Example: Google stock price (4/5)

```{r}
google_2018 |>
  autoplot(difference(Close)) +
  labs(y = "Change in Google closing stock price ($USD)")
```

## Example: Google stock price (5/5)

```{r}
google_2018 |> ACF(difference(Close)) |> autoplot()
```

## Differencing

* Differencing helps to **stabilize the mean**.
* The differenced series is the *change* between each observation in the original series: $y'_t = y_t - y_{t-1}$.
* The differenced series will have only $T-1$ values since it is not possible to calculate a difference $y_1'$ for the first observation.

## Random walk model {.smaller}
If differenced series is white noise with zero mean:


$y_t-y_{t-1}=\epsilon_t$ or $y_t=y_{t-1}+\epsilon_t$

where $\epsilon_t \sim NID(0,\sigma^2)$.

* Very widely used for non-stationary data.
* This is the model behind the **naÃ¯ve method**.
* Random walks typically have:
    * long periods of apparent trends up or down
    * Sudden/unpredictable changes in direction
* Forecast are equal to the last observation
    * future movements up or down are equally likely.

## Random walk with drift model {.smaller}

If differenced series is white noise with non-zero mean:

$y_t-y_{t-1}=c+\epsilon_t$ \hspace{0.4cm} or \hspace{0.4cm} $y_t=c+y_{t-1}+\epsilon_t$}

where $\epsilon_t \sim NID(0,\sigma^2)$.

* $c$ is the **average change** between consecutive observations.
* If $c>0$, $y_t$ will tend to drift upwards and vice versa.
* This is the model behind the **drift method**.

## Second-order differencing

Occasionally the differenced data will not appear stationary and it may be necessary to difference the data a second time:

. . .

$$
\begin{align*}
y''_{t} & = y'_{t} - y'_{t - 1} \\
        & = (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
        & = y_t - 2y_{t-1} +y_{t-2}.
\end{align*}
$$

* $y_t''$ will have $T-2$ values.
* In practice, it is almost never necessary to go beyond second-order differences.

## Seasonal differencing

A seasonal difference is the difference between an observation and the corresponding observation from the previous year.

$$
 y'_t = y_t - y_{t-m}
$$

where $m=$ number of seasons.

* For monthly data $m=12$.
* For quarterly data $m=4$.

## Antidiabetic drug sales (1/4)

```{r, echo=TRUE}
a10 <- PBS |>
  filter(ATC2 == "A10") |>
  summarise(Cost = sum(Cost)/1e6)
```

## Antidiabetic drug sales (2/4)

```{r, echo=TRUE}
a10 |> autoplot(
  Cost
)
```

## Antidiabetic drug sales (3/4)

```{r, echo=TRUE}
a10 |> autoplot(
  log(Cost)
)
```

## Antidiabetic drug sales (4/4)

```{r, echo=TRUE}
a10 |> autoplot(
  log(Cost) |> difference(12)
)
```

## Cortecosteroid drug sales (1/6)

```{r, echo=TRUE}
h02 <- PBS |>
  filter(ATC2 == "H02") |>
  summarise(Cost = sum(Cost)/1e6)
```

## Cortecosteroid drug sales (2/6)

```{r, echo=TRUE}
h02 |> autoplot(
  Cost
)
```

## Cortecosteroid drug sales (3/6)

```{r, echo=TRUE}
h02 |> autoplot(
  log(Cost)
)
```

## Cortecosteroid drug sales (4/6)

```{r, echo=TRUE}
h02 |> autoplot(
  log(Cost) |> difference(12)
)
```

## Cortecosteroid drug sales (5/6)

```{r, echo=TRUE}
h02 |> autoplot(
  log(Cost) |> difference(12) |> difference(1)
)
```

## Cortecosteroid drug sales (6/6){.smaller}

* Seasonally differenced series is closer to being stationary.
* Remaining non-stationarity can be removed with further first difference.

. . .

If $y'_t = y_t - y_{t-12}$ denotes seasonally differenced series, then twice-differenced series is


\begin{align*}
y^*_t &= y'_t - y'_{t-1} \\
      &= (y_t - y_{t-12}) - (y_{t-1} - y_{t-13}) \\
      &= y_t - y_{t-1} - y_{t-12} + y_{t-13}\: .
\end{align*}


## Seasonal differencing

When both seasonal and first differences are applied\dots

* it makes no difference which is done first---the result will be the same.
* If seasonality is strong, we recommend that seasonal differencing be done first because sometimes the resulting series will be stationary and there will be no need for further first difference.

. . .

It is important that if differencing is used, the differences are interpretable.

## Interpretation of differencing

* first differences are the change between **one observation and the next**;
* seasonal differences are the change between **one year to the next**.

. . .

But taking lag 3 differences for yearly data, for example, results in a model which cannot be sensibly interpreted.

## Unit root tests

**Statistical tests to determine the required order of differencing.**

  1. Augmented Dickey Fuller test: null hypothesis is that the data are non-stationary and non-seasonal.
  2. Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: null hypothesis is that the data are stationary and non-seasonal.
  3. Other tests available for seasonal data.

We will use the KPSS test. 

## KPSS test

```{r, echo=TRUE}
google_2018 |>
  features(Close, unitroot_kpss)
```

\pause

```{r, echo=TRUE}
google_2018 |>
  features(Close, unitroot_ndiffs)
```

## Automatically selecting differences (1/3)

STL decomposition: $y_t = T_t+S_t+R_t$

Seasonal strength $F_s = \max\big(0, 1-\frac{\text{Var}(R_t)}{\text{Var}(S_t+R_t)}\big)$

If $F_s > 0.64$, do one seasonal difference.

## Automatically selecting differences (2/3)

```{r, echo=TRUE}
h02 |> mutate(log_sales = log(Cost)) |>
  features(log_sales, list(unitroot_nsdiffs, feat_stl))
```

## Automatically selecting differences (3/3)


```{r, echo=TRUE}
h02 |> mutate(log_sales = log(Cost)) |>
  features(log_sales, unitroot_nsdiffs)
h02 |> mutate(d_log_sales = difference(log(Cost), 12)) |>
  features(d_log_sales, unitroot_ndiffs)
```

## Your turn

For the `tourism` dataset, compute the total number of trips and find an appropriate differencing (after transformation if necessary) to obtain stationary data.


## Backshift notation (1/4) {.smaller}

A very useful notational device is the backward shift operator, $B$, which is used as follows:

$$
  B y_{t} = y_{t - 1}
$$

. . .

In other words, $B$, operating on $y_{t}$, has the effect of **shifting the data back one period**. 

. . .

Two applications of $B$ to $y_{t}$ **shifts the data back two periods**:
$$
  B(By_{t}) = B^{2}y_{t} = y_{t-2}
$$

. . .

For monthly data, if we wish to shift attention to "the same month last year", then $B^{12}$ is used, and the notation is $B^{12}y_{t} = y_{t-12}$.

## Backshift notation (2/4) 

The backward shift operator is convenient for describing the process of *differencing*. 

A first-order difference can be written as
$$
  y'_{t} = y_{t} - y_{t-1} = y_t - By_{t} = (1 - B)y_{t}
$$

. . .

Similarly, if second-order differences (i.e., first differences of first differences) have to be computed, then:

$$y''_{t} = y_{t} - 2y_{t - 1} + y_{t - 2} = (1 - B)^{2} y_{t}$$


## Backshift notation (3/4) {.smaller}

* Second-order difference is denoted $(1- B)^{2}$.
* *Second-order difference* is not the same as a *second difference*, which would be denoted $1- B^{2}$;
* In general, a $d$th-order difference can be written as

. . .

$$
  (1 - B)^{d} y_{t}
$$

* A seasonal difference followed by a first difference can be written as

. . .

$$
  (1-B)(1-B^m)y_t
$$

## Backshift notation (4/4) 

The "backshift" notation is convenient because the terms can be multiplied together to see the combined effect.

\begin{align*}
  (1-B)(1-B^m)y_t & = (1 - B - B^m + B^{m+1})y_t \\
                  & = y_t-y_{t-1}-y_{t-m}+y_{t-m-1}.
\end{align*}

For monthly data, $m=12$ and we obtain the same result as earlier.

# Non-seasonal ARIMA models

## Autoregressive models {.smaller}

**Autoregressive (AR) models:**

$$
  y_{t} = c + \phi_{1}y_{t - 1} + \phi_{2}y_{t - 2} + \cdots + \phi_{p}y_{t - p} + \epsilon_{t},
$$
where $\epsilon_t$ is white noise. This is a multiple regression with **lagged values** of $y_t$ as predictors.


```{r arp, echo=FALSE, fig.height=2.35}
set.seed(1)
p1 <- tsibble(idx = seq_len(100), sim = 10 + arima.sim(list(ar = -0.8), n = 100), index = idx) |>
  autoplot(sim) + labs(y="", title  = "AR(1)")
p2 <- tsibble(idx = seq_len(100), sim = 20 + arima.sim(list(ar = c(1.3, -0.7)), n = 100), index = idx) |>
  autoplot(sim) + labs(y="", title ="AR(2)")
p1 | p2
```

## AR(1) model {.smaller}

$y_{t} = 18 -0.8 y_{t - 1} + \epsilon_{t},$ $\epsilon_t\sim N(0,1)$, $T=100$.

```{r, echo=FALSE}
p1
```

## AR(1) model {.smaller}


$y_{t} = c + \phi_1 y_{t - 1} + \epsilon_{t}$


* When $\phi_1=0$, $y_t$ is **equivalent to WN**
* When $\phi_1=1$ and $c=0$, $y_t$ is **equivalent to a RW**
* When $\phi_1=1$ and $c\ne0$, $y_t$ is **equivalent to a RW with drift**
* When $\phi_1<0$, $y_t$ tends to **oscillate between positive and negative values**.

## AR(2) model

$y_t = 8 + 1.3y_{t-1} - 0.7 y_{t-2} + \epsilon_t$ where $\epsilon_t\sim N(0,1)$ & $T=100$.

```{r, echo=FALSE}
p2
```

## Stationarity conditions

We normally restrict autoregressive models to stationary data, and then some constraints on the values of the parameters are required.

\begin{block}{General condition for stationarity
  Complex roots of $1-\phi_1 z - \phi_2 z^2 - \dots - \phi_pz^p$ lie outside the unit circle on the complex plane.
\end{block}\pause

* For $p=1$: $-1<\phi_1<1$.
* For $p=2$:\newline $-1<\phi_2<1\qquad \phi_2+\phi_1 < 1 \qquad \phi_2 -\phi_1 < 1$.
* More complicated conditions hold for $p\ge3$.
* Estimation software takes care of this.

## Moving Average (MA) models {.smaller}

**Moving Average (MA) models:**

$$
  y_{t} = c + \epsilon_t + \theta_{1}\varepsilon_{t - 1} + \theta_{2}\varepsilon_{t - 2} + \cdots + \theta_{q}\varepsilon_{t - q},
$$

where $\epsilon_t$ is white noise.
This is a multiple regression with **past *errors* ** as predictors. *Don't confuse this with moving average smoothing!*


```{r maq, fig.height=2.35, echo=FALSE}
set.seed(2)
p1 <- tsibble(idx = seq_len(100), sim = 20 + arima.sim(list(ma = 0.8), n = 100), index = idx) |>
  autoplot(sim) + labs(y="", title = "MA(1)")
p2 <- tsibble(idx = seq_len(100), sim = arima.sim(list(ma = c(-1, +0.8)), n = 100), index = idx) |>
  autoplot(sim) + labs(y="",title="MA(2)")

p1 | p2
```

## MA(1) model

$y_t = 20 + \epsilon_t + 0.8 \varepsilon_{t-1}$ where $\epsilon_t\sim N(0,1)$ & $T=100$.

```{r, echo=FALSE}
p1
```

## MA(2) model

$y_t = \epsilon_t -\varepsilon_{t-1} + 0.8 \varepsilon_{t-2}$ where $\epsilon_t\sim N(0,1)$ & $T=100$.

```{r, echo=FALSE}
p2
```

## MA($\infty$) models {.smaller}

It is possible to write any stationary AR($p$) process as an MA($\infty$) process.

**Example: AR(1)**
\begin{align*}
y_t &= \phi_1y_{t-1} + \epsilon_t\\
&= \phi_1(\phi_1y_{t-2} + \varepsilon_{t-1}) + \epsilon_t\\
&= \phi_1^2y_{t-2} + \phi_1 \varepsilon_{t-1} + \epsilon_t\\
&= \phi_1^3y_{t-3} + \phi_1^2\varepsilon_{t-2} + \phi_1 \varepsilon_{t-1} + \epsilon_t\\
&\dots
\end{align*}
Provided $-1<\phi_1<1$:
$$y_t = \epsilon_t + \phi_1\varepsilon_{t-1}+ \phi_1^2\varepsilon_{t-2}+ \phi_1^3\varepsilon_{t-3} + \cdots$$

## Invertibility

* Any MA($q$) process can be written as an AR($\infty$) process if we impose some constraints on the MA parameters.
* Then the MA model is called "invertible".
* Invertible models have some mathematical properties that make them easier to use in practice.

## Invertibility

**General condition for invertibility**
  Complex roots of $1+\theta_1 z + \theta_2 z^2 + \dots + \theta_qz^q$ lie outside the unit circle on the complex plane.

* For $q=1$: $-1<\theta_1<1$.
* For $q=2$:\newline $-1<\theta_2<1\qquad \theta_2+\theta_1 >-1 \qquad \theta_1 -\theta_2 < 1$.
* More complicated conditions hold for $q\ge3$.
* Estimation software takes care of this.

## ARMA models {.smaller}

**Autoregressive Moving Average models:**

$$y_{t} = c + \phi_{1}y_{t - 1} + \cdots + \phi_{p}y_{t - p} + \theta_{1}\varepsilon_{t - 1} + \cdots + \theta_{q}\varepsilon_{t - q} + \epsilon_{t}.$$


* Predictors include both **lagged values of $y_t$ and lagged errors.**
* Conditions on AR coefficients ensure stationarity.
* Conditions on MA coefficients ensure invertibility.

## Autoregressive Integrated Moving Average models

* Combine ARMA model with **differencing**.
* $(1-B)^d y_t$ follows an ARMA model.

## ARIMA models {.smaller}

**Autoregressive Integrated Moving Average models**

**ARIMA($p, d, q$) model**

| **Component** | **Description**                             |
|---------------|---------------------------------------------|
| **AR**        | $p =$ order of the autoregressive part    |
| **I**         | $d =$ degree of first differencing involved |
| **MA**        | $q =$ order of the moving average part    |

* White noise model: ARIMA(0,0,0)
* Random walk: ARIMA(0,1,0) with no constant
* Random walk with drift: ARIMA(0,1,0) with a constant (c)
* AR($p$): ARIMA($p$,0,0)
* MA($q$): ARIMA(0,0,$q$)

## Backshift notation for ARIMA {.smaller}

**ARMA model**

$$
\begin{aligned}
y_{t} &= c + \phi_{1}By_{t} + \cdots + \phi_pB^py_{t}
           + \epsilon_{t} + \theta_{1}B\epsilon_{t} + \cdots + \theta_qB^q\epsilon_{t} \\
\text{or}\quad
      & (1-\phi_1B - \cdots - \phi_p B^p) y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)\epsilon_t
\end{aligned}
$$

**ARIMA(1,1,1) model**

$$
\begin{array}{c c c c}
(1 - \phi_{1} B) & (1 - B) y_{t} &= &c + (1 + \theta_{1} B) \epsilon_{t}\\
{\uparrow} & {\uparrow} & &{\uparrow}\\
{\text{AR(1)}} & {\text{First}} &  &{\text{MA(1)}}\\
& \text{difference}\\
\end{array}
$$

. . .

$$
  y_t = c + y_{t-1} + \phi_1 y_{t-1}- \phi_1 y_{t-2} + \theta_1\varepsilon_{t-1} + \epsilon_t
$$
## General ARIMA Equation

\[
(1 - \phi_1 B - \cdots - \phi_p B^p)(1 - B)^d y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q) \varepsilon_t
\]

\[
\underbrace{(1 - \phi_1 B - \cdots - \phi_p B^p)}_{\text{AR}(p)}
\quad
\underbrace{(1 - B)^d}_{d \text{ differences}}
\quad
\underbrace{(1 + \theta_1 B + \cdots + \theta_q B^q)}_{\text{MA}(q)}
\]


## R model {.smaller}

**Intercept Form**

$(1-\phi_1B - \cdots - \phi_p B^p) y_t' = c + (1 + \theta_1 B + \cdots + \theta_q B^q)\epsilon_t$

. . .

**Mean Form**

$(1-\phi_1B - \cdots - \phi_p B^p)(y_t' - \mu) = (1 + \theta_1 B + \cdots + \theta_q B^q)\epsilon_t$

 * $y_t' = (1-B)^d y_t$
 * $\mu$ is the mean of $y_t'$.
 * $c = \mu(1-\phi_1 - \cdots - \phi_p )$.
 * `fable` uses intercept form

## Egyptian exports

```{r egyptexportsauto, echo=TRUE}
global_economy |>
  filter(Code == "EGY") |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Egyptian Exports")
```

## Egyptian exports

```{r, echo=TRUE, dependson="egyptexportsauto"}
fit <- global_economy |> filter(Code == "EGY") |>
  model(ARIMA(Exports))
report(fit)
```


```{r egyptexportsmodel, include=FALSE, warning=FALSE, dependson="egyptexportsauto"}
# stopifnot(identical(
#   unlist(fit[1,2][[1]][[1]]$fit$spec),
#   c(p=2L, d=0L, q=1L, P=0, D=0, Q=0, constant=TRUE, period.year=1)
# ))
coef <- rlang::set_names(tidy(fit)$estimate, tidy(fit)$term)
```

## ARIMA(2,0,1) model:
  
$$
  y_t = `r sprintf("%.2f", coef['constant'])`
         + `r sprintf("%.2f", coef['ar1'])` y_{t-1}
          `r sprintf("%.2f", coef['ar2'])` y_{t-2}
          `r sprintf("%.2f", coef['ma1'])` \varepsilon_{t-1}
          + \epsilon_{t},
$$

where $\epsilon_t$ is white noise with a standard deviation of $`r sprintf("%.3f", sqrt(broom::glance(fit)$sigma2))` = \sqrt{`r sprintf("%.3f", broom::glance(fit)$sigma2)`}$.

## Egyptian exports

```{r, echo=TRUE, fig.height=4}
gg_tsresiduals(fit)
```

## Egyptian exports


```{r, echo = TRUE}
augment(fit) |>
  features(.innov, ljung_box, lag = 10, dof = 4)
```

## Egyptian exports

```{r egyptexportsf, dependson="egyptexportsauto"}
fit |> forecast(h=10) |>
  autoplot(global_economy) +
  labs(y = "% of GDP", title = "Egyptian Exports")
```

## Understanding ARIMA models {.smaller}

* If $c=0$ and $d=0$, the long-term forecasts will go to zero.
* If $c=0$ and $d=1$, the long-term forecasts will go to a non-zero constant.
* If $c=0$ and $d=2$, the long-term forecasts will follow a straight line.

* If $c\ne0$ and $d=0$, the long-term forecasts will go to the mean of the data.
* If $c\ne0$ and $d=1$, the long-term forecasts will follow a straight line.
* If $c\ne0$ and $d=2$, the long-term forecasts will follow a quadratic trend.

## Understanding ARIMA models {.smaller}


**Forecast variance and $d$**

  * The higher the value of $d$, the more rapidly the prediction intervals increase in size.
  * For $d=0$, the long-term forecast standard deviation will go to the standard deviation of the historical data.

. . .

**Cyclic behaviour**

  * For cyclic forecasts, $p\ge2$ and some restrictions on coefficients are required.
  * If $p=2$, we need $\phi_1^2+4\phi_2<0$. Then average cycle of length $$(2\pi)/\left[\text{arc cos}(-\phi_1(1-\phi_2)/(4\phi_2))\right].$$


# Estimation and order selection

## Maximum likelihood estimation {.smaller}

Having identified the model order, we need to estimate the parameters $c$, $\phi_1,\dots,\phi_p$, $\theta_1,\dots,\theta_q$.

* MLE is very similar to least squares estimation obtained by minimizing

. . .

$$
  \sum_{t-1}^T e_t^2
$$

* The `ARIMA()` function allows LS or MLE estimation.
* Non-linear optimization must be used in either case.
* Different software will give different estimates.

## Partial autocorrelations {.smaller}

**Partial autocorrelations** measure relationship  
between $y_{t}$ and $y_{t - k}$, when the effects of other time lags --- $1, 2, 3, \dots, k - 1$ --- are removed.\pause

$$
\begin{aligned}
\alpha_k &= k\text{th partial autocorrelation coefficient}\\
         &= \text{equal to the estimate of } \phi_k \text{ in regression:}\\
         &  y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_k y_{t-k} + \epsilon_t.
\end{aligned}
$$

* Varying number of terms on RHS gives $\alpha_k$ for different values of $k$.
* $\alpha_1=\rho_1$
* same critical values of $\pm 1.96/\sqrt{T}$ as for ACF.
* Last significant $\alpha_k$ indicates the order of an AR model.

## Egyptian exports

```{r, eval=FALSE}
egypt <- global_economy |> filter(Code == "EGY")
egypt |> ACF(Exports) |> autoplot()
egypt |> PACF(Exports) |> autoplot()
```

```{r, echo=FALSE}
p1 <- global_economy |> filter(Code == "EGY") |> ACF(Exports) |> autoplot()
p2 <- global_economy |> filter(Code == "EGY") |> PACF(Exports) |> autoplot()
p1 | p2
```

## Egyptian exports

```{r, echo=TRUE, fig.height=3.25}
global_economy |> filter(Code == "EGY") |>
  gg_tsdisplay(Exports, plot_type='partial')
```

## ACF and PACF interpretation (1/4){.smaller}

**AR(1)**

$$
\begin{aligned}
  \rho_k &= \phi_1^k \text{ for } k=1,2,\dots; \\
  \alpha_1 &= \phi_1 \\
  \alpha_k &= 0 \text{ for } k=2,3,\dots.
\end{aligned}
$$

So we have an AR(1) model when

  * autocorrelations exponentially decay
  * there is a single significant partial autocorrelation.

## ACF and PACF interpretation (2/4){.smaller}

**AR($p$)**

  * ACF dies out in an exponential or damped sine-wave manner
  * PACF has all zero spikes beyond the $p$th spike

. . .

So we have an AR($p$) model when

  * the ACF is exponentially decaying or sinusoidal
  * there is a significant spike at lag $p$ in PACF, but none beyond $p$

## ACF and PACF interpretation (3/4)

**MA(1)**

$$
\begin{aligned}
\rho_1 &= \theta_1/(1 + \theta_1^2) \qquad \rho_k = 0 \qquad \text{for } k=2,3,\dots; \\
\alpha_k &= -(-\theta_1)^k/(1 + \theta_1^2 + \dots + \theta_1^{2k})
\end{aligned}
$$

So we have an MA(1) model when

* the PACF is exponentially decaying and
* there is a single significant spike in ACF

## ACF and PACF interpretation (4/4)

**MA($q$)**

* PACF dies out in an exponential or damped sine-wave manner
* ACF has all zero spikes beyond the $q$th spike

. . .

So we have an MA($q$) model when

* the PACF is exponentially decaying or sinusoidal
* there is a significant spike at lag $q$ in ACF, but none beyond $q$

## Information criteria {.smaller}

**Akaike's Information Criterion (AIC):**

$$
\text{AIC} = -2 \log(L) + 2(p+q+k+1),
$$

where $L$ is the likelihood of the data, $k=1$ if $c \ne 0$ and $k=0$ if $c=0$.\pause

**Corrected AIC:**

$$
\text{AICc} = \text{AIC} + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}.
$$\pause

**Bayesian Information Criterion:**

$$
\text{BIC} = \text{AIC} + \log(T)-2.
$$

. . .

Good models are obtained by minimizing either the AIC, **AICc** or **BIC**. Book authors' preference is to use the **AICc**.

# ARIMA modelling in R

## How does ARIMA() work? (1/7) {.smaller}

**A non-seasonal ARIMA process**

$$
\phi(B)(1-B)^d y_{t} = c + \theta(B) \epsilon_t
$$

. . .

Need to select appropriate orders: **$p, q, d$**

. . .


**Hyndman and Khandakar (JSS, 2008) algorithm:**

* Select no. differences **$d$** and **$D$** via KPSS test and seasonal strength measure.
* Select **$p, q$** by minimizing AICc.
* Use stepwise search to traverse model space.

## How does ARIMA() work? (2/7) {.smaller}

**AICc Calculation**

$$
\text{AICc} = -2 \log(L) + 2(p+q+k+1)\left[1 + \frac{(p+q+k+2)}{T-p-q-k-2}\right].
$$

where $L$ is the maximized likelihood fitted to the *differenced* data, $k=1$ if $c \ne 0$ and $k=0$ otherwise.

**Step 1:**

Select current model (with smallest AICc) from:

- ARIMA$(2,d,2)$
- ARIMA$(0,d,0)$
- ARIMA$(1,d,0)$
- ARIMA$(0,d,1)$

## How does ARIMA() work? (3/7) {.smaller}

**Step 2:**

Consider variations of current model:
- Vary one of $p, q$ from current model by $\pm1$
- $p, q$ both vary from current model by $\pm1$
- Include/exclude $c$ from current model

Model with lowest AICc becomes current model.

**Repeat Step 2 until no lower AICc can be found.**

## How does ARIMA() work? (4/7) {.smaller}

```{r ARMAgridsearch, echo=FALSE, message=FALSE, warning=FALSE, fig.asp=1, out.width="60%", fig.width=4, fig.height=4}
start <- tribble(
  ~p, ~q,
  0, 0,
  1, 0,
  0, 1,
  2, 2
)
selected <- tribble(
  ~p, ~q,
  2, 2,
  3, 3,
  4, 2
)
griddf <- expand.grid(p = 0:6, q = 0:6) |>
  as_tibble() |>
  left_join(start |> mutate(start = TRUE)) |>
  left_join(selected |> mutate(chosen = TRUE)) |>
  replace_na(list(start = FALSE, chosen = FALSE)) |>
  mutate(
    step = case_when(
      start ~ 1,
      (p - selected$p[1])^2 + (q - selected$q[1])^2 <= 2 ~ 2,
      (p - selected$p[2])^2 + (q - selected$q[2])^2 <= 2 ~ 3,
      (p - selected$p[3])^2 + (q - selected$q[3])^2 <= 2 ~ 4,
      TRUE ~ NA_real_
    )
  ) |>
  left_join(selected |>
              mutate(step = row_number() + 1) |>
              rename(fromp = p, fromq = q)
  ) |>
  mutate(step = as.character(step))
griddf |>
  ggplot(aes(x = q, y = p)) +
  geom_point(aes(alpha = 0.2), colour = "gray", size = 5, shape = 19) +
  geom_segment(aes(x = fromq, y = fromp, xend = q, yend = p, col=step),
               data = griddf |> filter(step %in% "1"),
               arrow = arrow(length = unit(0.15, "inches"), type='open'),
               size = 1, lineend = "butt") +
  geom_point(aes(col = step), size = 5, shape = 19,
             data = griddf |> filter(step=="1")) +
  geom_point(data = filter(griddf |> filter(step=="1"), chosen), size = 12, shape = 21, stroke = 1.4) +
  scale_y_reverse(breaks = 0:6) +
  scale_x_continuous(position = "top", breaks = 0:6) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title.x = element_text(size = 12, hjust = 0),
    axis.title.y = element_text(
      size = 12, hjust = 1,
      angle = 0, margin = margin(t = 0, r = 10, b = 0, l = 0)
    )
  ) +
  scale_colour_manual(
    breaks = paste(1:4),
    values = c("#D55E00", "#0072B2","#009E73", "#CC79A7")
  ) +
  guides(alpha = FALSE)
```

## How does ARIMA() work? (5/7) {.smaller}

```{r ARMAgridsearch2, echo=FALSE, message=FALSE, warning=FALSE, fig.asp=1, out.width="60%", fig.width=4, fig.height=4}
griddf |>
  ggplot(aes(x = q, y = p)) +
  geom_point(aes(alpha = 0.2), colour = "gray", size = 5, shape = 19) +
  geom_segment(aes(x = fromq, y = fromp, xend = q, yend = p, col=step),
               data = griddf |> filter(step=="2"),
               arrow = arrow(length = unit(0.15, "inches"), type='open'),
               size = 1, lineend = "butt") +
  geom_point(aes(col = step), size = 5, shape = 19,
             data = griddf |> filter(step %in% c("1","2"))) +
  geom_point(data = filter(griddf |> filter(step %in% c("1","2")), chosen), size = 12, shape = 21, stroke = 1.4) +
  scale_y_reverse(breaks = 0:6) +
  scale_x_continuous(position = "top", breaks = 0:6) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title.x = element_text(size = 12, hjust = 0),
    axis.title.y = element_text(
      size = 12, hjust = 1,
      angle = 0, margin = margin(t = 0, r = 10, b = 0, l = 0)
    )
  ) +
  scale_colour_manual(
    breaks = paste(1:4),
    values = c("#D55E00", "#0072B2","#009E73", "#CC79A7")
  ) +
  guides(alpha = FALSE)
```

## How does ARIMA() work? (6/7) {.smaller}

```{r ARMAgridsearch3, echo=FALSE, message=FALSE, warning=FALSE, fig.asp=1, out.width="60%", fig.width=4, fig.height=4}
griddf |>
  ggplot(aes(x = q, y = p)) +
  geom_point(aes(alpha = 0.2), colour = "gray", size = 5, shape = 19) +
  geom_segment(aes(x = fromq, y = fromp, xend = q, yend = p, col=step),
               data = griddf |> filter(step %in% "3"),
               arrow = arrow(length = unit(0.15, "inches"), type='open'),
               size = 1, lineend = "butt") +
  geom_point(aes(col = step), size = 5, shape = 19,
             data = griddf |> filter(step %in% c("1","2","3"))) +
  geom_point(data = griddf |> filter(step %in% c("1","2","3"), chosen),
             size = 12, shape = 21, stroke = 1.4) +
  scale_y_reverse(breaks = 0:6) +
  scale_x_continuous(position = "top", breaks = 0:6) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title.x = element_text(size = 12, hjust = 0),
    axis.title.y = element_text(
      size = 12, hjust = 1,
      angle = 0, margin = margin(t = 0, r = 10, b = 0, l = 0)
    )
  ) +
  scale_colour_manual(
    breaks = paste(1:4),
    values = c("#D55E00", "#0072B2","#009E73", "#CC79A7")
  ) +
  guides(alpha = FALSE)
```

## How does ARIMA() work? (7/7) {.smaller}

```{r ARMAgridsearch4, echo=FALSE, message=FALSE, warning=FALSE, fig.asp=1, out.width="60%", fig.width=4, fig.height=4}
griddf |>
  ggplot(aes(x = q, y = p)) +
  geom_point(aes(alpha = 0.2), colour = "gray", size = 5, shape = 19) +
  geom_segment(aes(x = fromq, y = fromp, xend = q, yend = p, col=step),
               data = griddf |> filter(step %in% "4"),
               arrow = arrow(length = unit(0.15, "inches"), type='open'),
               size = 1, lineend = "butt") +
  geom_point(aes(col = step), size = 5, shape = 19,
             data = griddf |> filter(step %in% c("1","2","3","4"))) +
  geom_point(data = griddf |> filter(step %in% c("1","2","3","4"), chosen),
             size = 12, shape = 21, stroke = 1.4) +
  scale_y_reverse(breaks = 0:6) +
  scale_x_continuous(position = "top", breaks = 0:6) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title.x = element_text(size = 12, hjust = 0),
    axis.title.y = element_text(
      size = 12, hjust = 1,
      angle = 0, margin = margin(t = 0, r = 10, b = 0, l = 0)
    )
  ) +
  scale_colour_manual(
    breaks = paste(1:4),
    values = c("#D55E00", "#0072B2","#009E73", "#CC79A7")
  ) +
  guides(alpha = FALSE)
```

## Egyptian exports

```{r, echo=TRUE, fig.height=3.25}
global_economy |> filter(Code == "EGY") |>
  gg_tsdisplay(Exports, plot_type='partial')
```

## Egyptian exports

```{r, echo=TRUE, fig.height=4}
fit1 <- global_economy |>
  filter(Code == "EGY") |>
  model(ARIMA(Exports ~ pdq(4,0,0)))
report(fit1)
```

## Egyptian exports


```{r, echo=TRUE, fig.height=4}
fit2 <- global_economy |>
  filter(Code == "EGY") |>
  model(ARIMA(Exports))
report(fit2)
```

## Central African Republic exports (1/8)

```{r}
global_economy |>
  filter(Code == "CAF") |>
  autoplot(Exports) +
  labs(title="Central African Republic exports", y="% of GDP")
```

## Central African Republic exports (2/8)

```{r caf2, warning=FALSE, fig.height=3}
global_economy |>
  filter(Code == "CAF") |>
  gg_tsdisplay(difference(Exports), plot_type='partial')
```

## Central African Republic exports (3/8)

```{r caf_fit}
caf_fit <- global_economy |>
  filter(Code == "CAF") |>
  model(arima210 = ARIMA(Exports ~ pdq(2,1,0)),
        arima013 = ARIMA(Exports ~ pdq(0,1,3)),
        stepwise = ARIMA(Exports),
        search = ARIMA(Exports, stepwise=FALSE))
```

## Central African Republic exports (4/8)


```{r caf_fit2}
caf_fit |> pivot_longer(!Country, names_to = "Model name",
                         values_to = "Orders")
```

## Central African Republic exports (5/8)


```{r caf_fit3, dependson=c("digits","caf_fit2")}
glance(caf_fit) |> arrange(AICc) |> select(.model:BIC)
```

## Central African Republic exports (6/8)


```{r cafres, dependson='caf_fit', fig.height=4}
caf_fit |> select(search) |> gg_tsresiduals()
```

## Central African Republic exports (7/8)


```{r caf_lb, dependson='caf_fit',}
augment(caf_fit) |>
  filter(.model=='search') |>
  features(.innov, ljung_box, lag = 10, dof = 3)
```

## Central African Republic exports (8/8)


```{r caffc, dependson="caf_fit"}
caf_fit |>
  forecast(h=5) |>
  filter(.model=='search') |>
  autoplot(global_economy)
```

## Modelling procedure with `ARIMA()` {.smaller}


1. Plot the data. Identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.
3. If the data are non-stationary: take first differences of the data until the data are stationary.
4. Examine the ACF/PACF: Is an AR($p$) or MA($q$) model appropriate?
5. Try your chosen model(s), and use the *AICc* to search for a better model.
6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.

## Automatic modelling procedure with `ARIMA()` {.smaller}

1. Plot the data. Identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.

. . .

3. Use `ARIMA` to automatically select a model.

. . .

6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.

## Modelling procedure

<iframe src="figs/figure_8_10.pdf" width="100%" height="600px"></iframe>


# Forecasting

## Prediction intervals {.smaller}

**95% prediction interval**

$$
\hat{y}_{T+h|T} \pm 1.96\sqrt{v_{T+h|T}}
$$

where $v_{T+h|T}$ is estimated forecast variance.

## Prediction intervals {.smaller}

Multi-step prediction intervals for ARIMA(0,0,$q$):


$$
y_t = \epsilon_t + \sum_{i=1}^q \theta_i \varepsilon_{t-i}.
$$

$$
v_{T|T+h} = \hat{\sigma}^2 \left[ 1 + \sum_{i=1}^{h-1} \theta_i^2\right], \qquad \text{for } h=2,3,\dots.
$$

. . .

* AR(1): Can be rewritten as MA($\infty$) and use above result.
* Other models beyond scope of this subject.

## Prediction intervals {.smaller}

* Prediction intervals **increase in size with forecast horizon**.
* Prediction intervals can be difficult to calculate by hand
* Calculations assume residuals are **uncorrelated** and **normally distributed**.
* Prediction intervals tend to be too narrow.
    * the uncertainty in the parameter estimates has not been accounted for.
    * the ARIMA model assumes historical patterns will not change during the forecast period.
    * the ARIMA model assumes uncorrelated future errors

## Your turn

For the GDP data (from `global_economy`):

 * fit a suitable ARIMA model to the logged data for all countries
 * check the residual diagnostics for Australia;
 * produce forecasts of your fitted model for Australia.





# Backshift notation revisited

## Recall Backshift notation {.smaller}

Backward shift operator, $B$, which is used as follows:
$$
B y_{t} = y_{t - 1} \: .
$$

. . .

$B$, operating on $y_{t}$, has the effect of **shifting the data back one period**.

. . .

For monthly data, if we wish to shift attention to "the same month last year," then $B^{12}$ is used, and the notation is $B^{12}y_{t} = y_{t-12}$.


# Seasonal ARIMA models

## Seasonal ARIMA models 

| ARIMA | $~\underbrace{(p, d, q)}$ | $\underbrace{(P, D, Q)_{m}}$ |
| ----: | :-----------------------: | :--------------------------: |
|       | ${\uparrow}$              | ${\uparrow}$                 |
|       | Non-seasonal part         | Seasonal part of             |
|       | of the model              | of the model                 |

where $m =$ number of observations per year.

## ARIMA$(1, 1, 1)(1, 1, 1)_{4}$ model (without constant) {.smaller}


$$
\begin{aligned}
&\underbrace{(1 - \phi_1 B)}_{\text{Non-seasonal AR(1)} \phantom{X}}
\underbrace{(1 - \Phi_1 B^4)}_{\text{Seasonal AR(1)} \phantom{X}}
\underbrace{(1 - B)}_{\text{Non-seasonal difference} \phantom{X}}
\underbrace{(1 - B^4)}_{\text{Seasonal difference} \phantom{X}}
y_t = \\
&\underbrace{(1 + \theta_1 B)}_{\text{Non-seasonal MA(1)} \phantom{X}}
\underbrace{(1 + \Theta_1 B^4)}_{\text{Seasonal MA(1)} \phantom{X}}
\varepsilon_t
\end{aligned}
$$


## Seasonal ARIMA models {.smaller}

E.g., ARIMA$(1, 1, 1)(1, 1, 1)_{4}$ model (without constant)

$$(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} =(1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\epsilon_{t}.$$


## Common ARIMA models

The US Census Bureau uses the following models most often:

| Model                        | Transformation          |
|------------------------------|-------------------------|
| ARIMA(0,1,1)(0,1,1)$_m$      | with log transformation |
| ARIMA(0,1,2)(0,1,1)$_m$      | with log transformation |
| ARIMA(2,1,0)(0,1,1)$_m$      | with log transformation |
| ARIMA(0,2,2)(0,1,1)$_m$      | with log transformation |
| ARIMA(2,1,2)(0,1,1)$_m$      | with no transformation  |

## Seasonal ARIMA models {.smaller}

The seasonal part of an AR or MA model will be seen in the seasonal lags of
the PACF and ACF.

**ARIMA(0,0,0)(0,0,1)$_{12}$ will show:**

  * a spike at lag 12 in the ACF but no other significant spikes.
  * The PACF will show exponential decay in the seasonal lags; that is, at lags 12, 24, 36, \dots.

. . .

**ARIMA(0,0,0)(1,0,0)$_{12}$ will show:**

  * exponential decay in the seasonal lags of the ACF
  * a single significant spike at lag 12 in the PACF.

## US leisure employment (1/8)

```{r, fig.height=4}
leisure <- us_employment |>
  filter(Title == "Leisure and Hospitality", year(Month) > 2000) |>
  mutate(Employed = Employed/1000) |>
  select(Month, Employed)
autoplot(leisure, Employed) +
  labs(title = "US employment: leisure & hospitality", y="People (millions)")
```

## US leisure employment (2/8)

```{r, fig.height=4}
leisure |>
  gg_tsdisplay(difference(Employed, 12), plot_type='partial', lag=36) +
  labs(title="Seasonally differenced", y="")
```

## US leisure employment (3/8)

```{r, fig.height=4}
leisure |>
  gg_tsdisplay(difference(Employed, 12) |> difference(),
    plot_type='partial', lag=36) +
  labs(title = "Double differenced", y="")
```

## US leisure employment (4/8)


```{r}
fit <- leisure |>
  model(
    arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
    arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
    auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
  )
fit |> pivot_longer(everything(), names_to = "Model name",
                     values_to = "Orders")
```

## US leisure employment (5/8)


```{r}
glance(fit) |> arrange(AICc) |> select(.model:BIC)
```

## US leisure employment (6/8)

```{r, fig.height=4}
fit |> select(auto) |> gg_tsresiduals(lag=36)
```

## US leisure employment (7/8)


```{r}
augment(fit) |> features(.innov, ljung_box, lag=24, dof=4)
```

## US leisure employment (8/8)

```{r}
forecast(fit, h=36) |>
  filter(.model=='auto') |>
  autoplot(leisure) +
  labs(title = "US employment: leisure and hospitality", y="Number of people (millions)")
```

## Cortecosteroid drug sales (1/20)

```{r}
h02 <- PBS |>
  filter(ATC2 == "H02") |>
  summarise(Cost = sum(Cost)/1e6)
```

## Cortecosteroid drug sales (2/20)

```{r}
h02 |> autoplot(
  Cost
)
```

## Cortecosteroid drug sales (3/20)

```{r}
h02 |> autoplot(
  log(Cost)
)
```

## Cortecosteroid drug sales (4/20)

```{r}
h02 |> autoplot(
  log(Cost) |> difference(12)
)
```

## Cortecosteroid drug sales (5/20)

```{r h02b, fig.height=4}
h02 |> gg_tsdisplay(difference(log(Cost),12),
                 lag_max = 36, plot_type = 'partial')
```

## Cortecosteroid drug sales (6/20)

  * Choose $D=1$ and $d=0$.
  * Spikes in PACF at lags 12 and 24 suggest seasonal AR(2) term.
  * Spikes in PACF suggests possible non-seasonal AR(3) term.
  * Initial candidate model: ARIMA(3,0,0)(2,1,0)$_{12}$.

## Cortecosteroid drug sales  (7/20){.smaller}

```{r h02aicc, echo=FALSE}
models <- list(
  c(3,0,0,2,1,0),
  c(3,0,1,2,1,0),
  c(3,0,2,2,1,0),
  c(3,0,1,1,1,0),
  c(3,0,1,0,1,1),
  c(3,0,1,0,1,2),
  c(3,0,1,1,1,1)
)
model_defs <- map(models, ~ ARIMA(log(Cost) ~ 0 + pdq(!!.[1], !!.[2], !!.[3]) + PDQ(!!.[4], !!.[5], !!.[6])))
model_defs <- set_names(model_defs, map_chr(models,
  ~ sprintf("ARIMA(%i,%i,%i)(%i,%i,%i)[12]", .[1], .[2], .[3], .[4], .[5], .[6])))

fit <- h02 |>
  model(!!!model_defs)

fit |>
  glance() |>
  arrange(AICc) |>
  select(.model, AICc) |>
  knitr::kable(digits=2, row.names=FALSE, align='cc', booktabs=TRUE)
```

## Cortecosteroid drug sales (8/20)


```{r arimah02, echo=TRUE}
fit <- h02 |>
  model(best = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2)))
report(fit)
```

## Cortecosteroid drug sales (9/20)

```{r h02res, echo=TRUE, fig.height=4, dependson='arimah02'}
gg_tsresiduals(fit)
```

## Cortecosteroid drug sales (10/20)


```{r h02resb, echo = TRUE, fig.height=4, dependson='arimah02'}
augment(fit) |>
  features(.innov, ljung_box, lag = 36, dof = 6)
```

## Cortecosteroid drug sales (11/20)


```{r h02auto, echo=TRUE, fig.height=3.6}
fit <- h02 |> model(auto = ARIMA(log(Cost)))
report(fit)
```

## Cortecosteroid drug sales (12/20)

```{r, echo=TRUE, fig.height=4, dependson='h02auto'}
gg_tsresiduals(fit)
```

## Cortecosteroid drug sales (13/20)


```{r, echo = TRUE, dependson='h02auto'}
augment(fit) |>
  features(.innov, ljung_box, lag = 36, dof = 3)
```

## Cortecosteroid drug sales (14/20)


```{r h02tryharder, echo=TRUE, fig.height=3.6}
fit <- h02 |>
  model(best = ARIMA(log(Cost), stepwise = FALSE,
                 approximation = FALSE,
                 order_constraint = p + q + P + Q <= 9))
report(fit)
```

## Cortecosteroid drug sales (15/20)

```{r, echo=TRUE, fig.height=4, dependson='h02tryharder'}
gg_tsresiduals(fit)
```

## Cortecosteroid drug sales (16/20)


```{r, echo = TRUE, dependson='h02tryharder'}
augment(fit) |>
  features(.innov, ljung_box, lag = 36, dof = 9)
```

## Cortecosteroid drug sales (17/20){.smaller}


Training data: July 1991 to June 2006

Test data: July 2006--June 2008

```r
fit <- h02 |>
  filter_index(~ "2006 Jun") |>
  model(
    ARIMA(log(Cost) ~ 0 + pdq(3, 0, 0) + PDQ(2, 1, 0)),
    ARIMA(log(Cost) ~ 0 + pdq(3, 0, 1) + PDQ(2, 1, 0)),
    ARIMA(log(Cost) ~ 0 + pdq(3, 0, 2) + PDQ(2, 1, 0)),
    ARIMA(log(Cost) ~ 0 + pdq(3, 0, 1) + PDQ(1, 1, 0))
    # ... #
  )

fit |>
  forecast(h = "2 years") |>
  accuracy(h02)
```

## Cortecosteroid drug sales (18/20){.smaller}


```{r h02-rmse, cache=TRUE, echo=FALSE}
models <- list(
  c(3,0,1,0,1,2),
  c(3,0,1,1,1,1),
  c(3,0,1,0,1,1),
  c(3,0,1,2,1,0),
  c(3,0,0,2,1,0),
  c(3,0,2,2,1,0),
  c(3,0,1,1,1,0),
  c(2,1,0,0,1,1),
  c(4,1,1,2,1,2))

model_defs <- map(models, ~ ARIMA(log(Cost) ~ 0 + pdq(!!.[1], !!.[2], !!.[3]) + PDQ(!!.[4], !!.[5], !!.[6])))
model_defs <- set_names(model_defs, map_chr(models,
  ~ sprintf("ARIMA(%i,%i,%i)(%i,%i,%i)[12]", .[1], .[2], .[3], .[4], .[5], .[6])))

fit <- h02 |>
  filter_index(~ "2006 Jun") |>
  model(!!!model_defs)

fit |>
  forecast(h = "2 years") |>
  accuracy(h02) |>
  arrange(RMSE) |>
  select(.model, RMSE) |>
  knitr::kable(digits=4)
```

## Cortecosteroid drug sales (19/20)

  * Models with lowest AICc values tend to give slightly better results than the other models.
  * AICc comparisons must have the same orders of differencing. But RMSE test set comparisons can involve any models.
  * Use the best model available, even if it does not pass all tests.

## Cortecosteroid drug sales (20/20)

```{r h02f, echo=TRUE}
fit <- h02 |>
  model(ARIMA(Cost ~ 0 + pdq(3,0,1) + PDQ(0,1,2)))
fit |> forecast() |> autoplot(h02) +
  labs(y = "H02 Expenditure ($AUD)")
```

# ARIMA vs ETS

## ARIMA vs ETS {.smaller}


  * Myth that ARIMA models are more general than exponential smoothing.
  * Linear exponential smoothing models all special cases of ARIMA models.
  * Non-linear exponential smoothing models have no equivalent ARIMA counterparts.
  * Many ARIMA models have no exponential smoothing counterparts.
  * ETS models all non-stationary. Models with seasonality or non-damped trend (or both) have two unit roots; all other models have one unit **root.**



## ARIMA vs ETS

```{r venn, echo=FALSE}
library(latex2exp)
cols = c(ets = "#D55E00", arima = "#0072b2")
tibble(
    x = c(-1, 1),
    y = c(-0.5, -0.5),
    labels = c("ets", "arima"),
  ) |>
  ggplot(aes(colour = labels, fill=labels)) +
  ggforce::geom_circle(aes(x0 = x, y0 = y, r = 1.6), alpha = 0.3, size = 1) +
  scale_colour_manual(values=cols) + scale_fill_manual(values=cols) +
  coord_fixed() + guides(fill = "none") +
  geom_text(aes(label = "ETS models", x = -1.5, y = 1.35), col = cols["ets"], fontface = "bold", size=6) +
  geom_text(aes(label = "Combination\n of components", x = -1.3, y = 0.5), col = cols["ets"], size=2.8, fontface = "bold") +
  geom_text(aes(label = "9 ETS models with\n multiplicative errors", x = -1.6, y = -0.5), col = cols["ets"], size=2.8) +
  geom_text(aes(label = "3 ETS models with\n additive errors and\n multiplicative\n seasonality", x = -1.3, y = -1.4), col = cols["ets"], size=2.8) +
  geom_text(aes(label = "ARIMA models", x = 1.5, y = 1.35), col = cols["arima"], fontface = "bold", size=6) +
  geom_text(aes(label = "Modelling\n autocorrelations", x = 1.3, y = 0.5), col = cols["arima"], size=2.8, fontface = "bold") +
  annotate("text", label = TeX("Potentially $\\infty$ models"), x = 1.6, y = -0.6, col = cols["arima"], size=2.8) +
  geom_text(aes(label = "All stationary models\n Many large models", x = 1.25, y = -1.5), col = cols["arima"], size=2.8) +
  geom_text(aes(label = "6 fully additive\n ETS models", x = 0, y = -0.6), col = "#6b6859", size=2.8) +
  guides(colour = "none", fill = "none") +  theme_void()
```

## Equivalences {.smaller}



|**ETS model**  | **ARIMA model**             | **Parameters**                       |
| :------------ | :-------------------------- | :----------------------------------- |
| ETS(A,N,N)    | ARIMA(0,1,1)                | $\theta_1 = \alpha-1$                |
| ETS(A,A,N)    | ARIMA(0,2,2)                | $\theta_1 = \alpha+\beta-2$          |
|               |                             | $\theta_2 = 1-\alpha$                |
| ETS(A,A\damped,N)    | ARIMA(1,1,2)                | $\phi_1=\phi$                        |
|               |                             | $\theta_1 = \alpha+\phi\beta-1-\phi$ |
|               |                             | $\theta_2 = (1-\alpha)\phi$          |
| ETS(A,N,A)    | ARIMA(0,0,$m$)(0,1,0)$_m$   |                                      |
| ETS(A,A,A)    | ARIMA(0,1,$m+1$)(0,1,0)$_m$ |                                      |
| ETS(A,A\damped,A)    | ARIMA(1,0,$m+1$)(0,1,0)$_m$ |                                      |

## Example: Australian population


```{r tscvpop, echo=TRUE, warning=FALSE}
aus_economy <- global_economy |> filter(Code == "AUS") |>
  mutate(Population = Population/1e6)
aus_economy |>
  slice(-n()) |>
  stretch_tsibble(.init = 10) |>
  model(ets = ETS(Population),
        arima = ARIMA(Population)
  ) |>
  forecast(h = 1) |>
  accuracy(aus_economy) |>
  select(.model, ME:RMSSE)
```

## Example: Australian population

```{r popetsplot, echo=TRUE, fig.height=2.6}
aus_economy |>
  model(ETS(Population)) |>
  forecast(h = "5 years") |>
  autoplot(aus_economy) +
  labs(title = "Australian population",  y = "People (millions)")
```

## Example: Cement production (1/9)

```{r qcement1, echo=TRUE}
cement <- aus_production |>
  select(Cement) |>
  filter_index("1988 Q1" ~ .)
train <- cement |> filter_index(. ~ "2007 Q4")
fit <- train |>
  model(
    arima = ARIMA(Cement),
    ets = ETS(Cement)
  )
```

## Example: Cement production (2/9)


```{r qcement2, dependson="qcement1"}
fit |>
  select(arima) |>
  report()
```

## Example: Cement production (3/9)


```{r qcement3, dependson="qcement1"}
fit |> select(ets) |> report()
```

## Example: Cement production (4/9)

```{r qcement4, dependson="qcement1", fig.height=4}
gg_tsresiduals(fit |> select(arima), lag_max = 16)
```

## Example: Cement production (5/9)

```{r qcement5, dependson="qcement1", fig.height=4}
gg_tsresiduals(fit |> select(ets), lag_max = 16)
```

## Example: Cement production (6/9)


```{r qcement6, dependson="qcement1"}
fit |>
  select(arima) |>
  augment() |>
  features(.innov, ljung_box, lag = 16, dof = 6)
```

## Example: Cement production (7/9)


```{r qcement7, dependson="qcement1"}
fit |>
  select(ets) |>
  augment() |>
  features(.innov, ljung_box, lag = 16, dof = 6)
```

## Example: Cement production (8/9)


```{r qcement8, dependson=c("qcement2","qcement3")}
fit |>
  forecast(h = "2 years 6 months") |>
  accuracy(cement) |>
  select(-ME, -MPE, -ACF1)
```

## Example: Cement production (9/9)

```{r qcement9, echo=TRUE, dependson="qcement1", fig.height=2.6}
fit |>
  select(arima) |>
  forecast(h="3 years") |>
  autoplot(cement) +
  labs(title = "Cement production in Australia", y="Tonnes ('000)")
```
