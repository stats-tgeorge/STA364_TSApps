---
title: "STA 364 Time Series Applications"
author: "Ch3. Time series decomposition"
date: "OTexts.org/fpp3/"
classoption: aspectratio=169
titlecolor: cornellpurple
toc: true
output:
  binb::monash:
    colortheme: monashwhite
    fig_width: 7.5
    fig_height: 3
    keep_tex: no
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
source("setup.R")
library(purrr)
library(transformr) # Just to get it on renv
library(gganimate)
library(latex2exp)
```

# Transformations and adjustments

## Per capita adjustments

```{r gdp-per-capita}
global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(GDP)
```

## Per capita adjustments

```{r gdp-per-capita2}
global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(GDP / Population)
```

<!--
## Your turn

Consider the GDP information in `global_economy`. Plot the GDP per capita for each country over time. Which country has the highest GDP per capita? How has this changed over time? -->

## Inflation adjustments

- Data which are affected by the value of money are best adjusted before modelling. 
 - For example, the average cost of a new house will have increased over the last few decades due to inflation. A \$200,000 house this year is not the same as a \$200,000 house twenty years ago. 

## Inflation adjustments
- To make these adjustments, a price index is used. If $z_t$ denotes the price index and $y_t$ denotes the original house price in year $t$, then 

$$x_t=y_t/z_t \cdot z_{2000}$$

gives the adjusted house price at year 2000 dollar values. 

- Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).

## Inflation adjustments

```{r retail_cpi, message=FALSE, warning=FALSE, fig.show='hide'}
aus_newspaper_retail <- readr::read_csv('data/aus_newspaper_retail.csv')
# aus_newspaper_retail <- 
#       readr::read_csv('~/STA364/data/aus_newspaper_retail.csv')
# Turnover:	Retail turnover in $Million AUD

aus_newspaper_retail <- aus_newspaper_retail %>%
  select(Year,Turnover,name) %>% 
  group_by(Year,name) %>%
  summarise(sum_Turnover = sum(Turnover))%>%
  as_tsibble(index = Year, key = "name")

aus_newspaper_retail %>% autoplot(sum_Turnover) +
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry", y = "$AU")

```

## Inflation adjustments

```{r ref.label = 'retail_cpi', message=FALSE, warning=FALSE, echo=FALSE, fig.height=4, fig.width=8}
```

## Mathematical transformations

\fontsize{13}{15}\sf

If the data show different variation at different levels of the series, then a transformation can be useful.
\pause

Denote original observations as $y_1,\dots,y_T$ and transformed
observations as $w_1, \dots, w_T$.
\pause

\begin{block}{\footnotesize Mathematical transformations for stabilizing
variation}
\begin{tabular}{llc}
Square root & $w_t = \sqrt{y_t}$ & $\downarrow$ \\[0.2cm]
Cube root & $w_t = \sqrt[3]{y_t}$ & Increasing \\[0.2cm]
Logarithm & $w_t = \log(y_t)$  & strength
\end{tabular}
\end{block}
\pause

Logarithms, in particular, are useful because they are more interpretable: changes in a log value are **relative (percent) changes on the original scale**.

## Mathematical transformations

```{r food}
food <- aus_retail %>%
  filter(Industry == "Food retailing") %>%
  summarise(Turnover = sum(Turnover))
```

```{r food-plot, echo = FALSE}
food %>% autoplot(Turnover) +
  labs(y = "Turnover ($AUD)")
```

## Mathematical transformations

```{r food-sqrt1}
food %>% autoplot(sqrt(Turnover)) +
  labs(y = "Square root turnover")
```

## Mathematical transformations

```{r food-cbrt}
food %>% autoplot(Turnover^(1/3)) +
  labs(y = "Cube root turnover")
```

## Mathematical transformations

```{r food-log}
food %>% autoplot(log(Turnover)) +
  labs(y = "Log turnover")
```

(log here is the natural log, base $e$)

## Box-Cox transformations

Each of these transformations is close to a member of the
family of \textbf{Box-Cox transformations}:
$$w_t = \left\{\begin{array}{ll}
        \log(y_t),      & \quad \lambda = 0; \\
        (sign(y_t)|y_t|^\lambda-1)/\lambda ,         & \quad \lambda \ne 0.
\end{array}\right.
$$\pause

- Actually the Bickel-Doksum transformation (allowing for $y_t<0$)
- $\lambda=1$: (No substantive transformation)
- $\lambda=\frac12$: (Square root plus linear transformation)
- $\lambda=0$: (Natural logarithm)

## Box-Cox transformations

```{r food-anim, cache=TRUE, echo=FALSE, fig.show='animate', interval=1/10, message=FALSE, fig.height=4.5, fig.width=9, aniopts='controls,buttonsize=0.3cm,width=13.5cm'}
food %>%
  mutate(!!!set_names(map(seq(0, 1, 0.01), ~ expr(fabletools::box_cox(Turnover, !!.x))), seq(0, 1, 0.01))) %>%
  select(-Turnover) %>%
  pivot_longer(-Month, names_to = "lambda", values_to = "Turnover") %>%
  mutate(lambda = as.numeric(lambda)) %>%
  ggplot(aes(x = Month, y = Turnover)) +
  geom_line() +
  transition_states(1 - lambda, state_length = 0) +
  view_follow() +
  labs(title = "Box-Cox transformed food retailing turnover (lambda = {format(1 - as.numeric(closest_state), digits = 2)})")
```

## Box-Cox transformations

```{r food-lambda}
food %>%
  features(Turnover, features = guerrero)
```

\pause\fontsize{13}{15}\sf

- This attempts to balance the seasonal fluctuations and random variation across the series.
- Always check the results.
- A low value of $\lambda$ can give extremely large prediction intervals.

## Box-Cox transformations

```{r food-bc}
food %>% autoplot(box_cox(Turnover, 0.0524)) +
  labs(y = "Box-Cox transformed turnover")
```

## Transformations

\fontsize{13}{15}\sf

- Often no transformation needed.
- Simple transformations are easier to explain and work well enough.
- Transformations can have very large effect on PI.
- If some data are zero or negative, then use $\lambda>0$.
- `log1p()` can also be useful for data with zeros.
- Choosing logs is a simple way to force forecasts to be positive
- Transformations must be reversed to obtain forecasts on the original scale. (Handled automatically by `fable`.)

<!-- ## Your turn

\fontsize{13}{14}\sf

1. For the following series, find an appropriate transformation in order to stabilise the variance.

    - United States GDP from `global_economy`
    - Slaughter of Victorian “Bulls, bullocks and steers” in `aus_livestock`
    - Victorian Electricity Demand from `vic_elec`.
    - Gas production from `aus_production`

2. Why is a Box-Cox transformation unhelpful for the `canadian_gas` data?
 -->

# Time series components

## Time series patterns

**Recall**

Trend
:  pattern exists when there is a long-term increase or decrease in the data.

Cyclic
: pattern exists when data exhibit rises and falls that are *not of fixed period* (duration usually of at least 2 years).

Seasonal
: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).

## Time series decomposition

\begin{block}{}\vspace{-0.3cm}
\[ y_t = f(S_t, T_t, R_t) \]
\end{block}
\begin{tabular}{@{}llp{8cm}@{}}
where & $y_t=$ & data at period $t$ \\
      & $T_t=$ & trend-cycle component at period $t$\\
      & $S_t=$ & seasonal component at period $t$ \\
      & $R_t=$ & remainder component at period $t$
\end{tabular}
\pause

**Additive decomposition:** $y_t = S_t + T_t + R_t.$

**Multiplicative decomposition:** $y_t = S_t \times T_t \times R_t.$

## Time series decomposition

\fontsize{13}{15}\sf

  -  Additive model  appropriate if  magnitude of  seasonal fluctuations does not vary with level.
  -  If seasonal are proportional to level of series, then multiplicative model appropriate.
  -  Multiplicative decomposition more prevalent with economic series
  -  Alternative: use a Box-Cox transformation, and then use additive decomposition.
  -  Logs turn multiplicative relationship into an additive relationship:

$$y_t = S_t \times T_t \times R_t \quad\Rightarrow\quad
\log y_t = \log S_t + \log T_t + \log R_t.$$

## US Retail Employment

\fontsize{11}{11}\sf

```{r usretail}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade") %>%
  select(-Series_ID)
us_retail_employment
```

\vspace{10cm}

## US Retail Employment

```{r dable1}
us_retail_employment %>%
  autoplot(Employed) +
  labs(y="Persons (thousands)", title="Total employment in US retail")
```

\vspace{10cm}

## US Retail Employment

\fontsize{11}{11}\sf

```{r dable2}
us_retail_employment %>%
  model(stl = STL(Employed))
```

\vspace{10cm}

## US Retail Employment

\fontsize{11}{11}\sf

```{r dable3}
dcmp <- us_retail_employment %>%
  model(stl = STL(Employed))
components(dcmp)
```

\vspace{10cm}

## US Retail Employment

```{r dable4}
us_retail_employment %>%
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), trend, color='#523178') +
  labs(y="Persons (thousands)", title="Total employment in US retail")
```

\vspace{10cm}

## US Retail Employment

```{r usretail-stl, fig.width=8, fig.height=5}
components(dcmp) %>% autoplot()
```

## US Retail Employment

```{r usretail3}
components(dcmp) %>% gg_subseries(season_year)
```

## Seasonal adjustment

  -  Useful by-product of decomposition:  an easy way to calculate seasonally adjusted data.
  -  Additive decomposition: seasonally adjusted data given by
$$y_t - S_t = T_t + R_t$$
  -  Multiplicative decomposition: seasonally adjusted data given by
$$y_t / S_t = T_t \times R_t$$

## US Retail Employment

```{r usretail-sa}
us_retail_employment %>%
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), season_adjust, color='#0072B2') +
  labs(y="Persons (thousands)", title="Total employment in US retail")
```

## Seasonal adjustment

  - We use estimates of $S$ based on past values to seasonally adjust a current value.
  -  Seasonally adjusted series reflect **remainders** as well as **trend**. Therefore they are not "smooth" and "downturns" or "upturns" can be misleading.
  -  It is better to use the trend-cycle component to look for turning points.

# Moving Averages (MA)

## Moving Averages

- The classical method of time series decomposition originated in the 1920s and was widely used until the 1950s. 
- A moving average of order $m$ can be written as 
$$\hat{T}_t = \frac{1}{m}\Sigma_{j=-k}^k y_{t+j}$$
where $m = 2k+1$. 
- That is, the estimate of the trend-cycle at time $t$ is obtained by averaging values of the time series within $k$ periods of $t$.


## MA

- Averaging eliminates some of the randomness in the data, leaving a smooth trend-cycle component. 
- Called an m-MA, or MA(m), meaning a moving average of order m.

## MA global economy

```{r globa_econ_MA}
global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")
```

## MA global economy

- A moving average is a an average that moves...
- MA(5) would mean we use the average of two time series values before $y_t$, $y_t$ itself, and the two values after $y_t$.
 - Ex: to get the MA(5) for $y_{100}$ we would use the average of $y_{98},y_{99},y_{100},y_{101}$ and $y_{102}$. 
- What is a limitation of this method? 

## MA global economy sliding window

- slide_dbl() from the slider package applies a function to “sliding” time windows. In this case, we use the mean() function with a window of size 5.
```{r globa_econ_sliding_window}
aus_exports <- global_economy %>%
  filter(Country == "Australia") %>%
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE)
  )
```

## MA global economy MA trend line
```{r globa_econ_MA_line, warning=FALSE, fig.width=8.5, fig.height=3.4}
aus_exports %>%
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports") 
```


# History of time series decomposition

## History of time series decomposition

\fontsize{13}{15}\sf

  -  Classical method originated in 1920s.
  -  Census II method introduced in 1957. Basis for X-11 method and variants (including X-12-ARIMA, X-13-ARIMA)
  -  STL method introduced in 1983
  -  TRAMO/SEATS introduced in 1990s.
\pause

### National Statistics Offices
 - ABS uses X-12-ARIMA
 - US Census Bureau uses X-13ARIMA-SEATS
 - Statistics Canada uses X-12-ARIMA
 - ONS (UK) uses X-12-ARIMA
 - EuroStat use X-13ARIMA-SEATS

## X-11 decomposition

**Advantages**

  -  Relatively robust to outliers
  -  Completely automated choices for trend and seasonal changes
  -  Very widely tested on economic data over a long period of time.

\pause

**Disadvantages**

  -  No prediction/confidence intervals
  -  Ad hoc method with no underlying model
  -  Only developed for quarterly and monthly data

## Extensions: X-12-ARIMA and X-13-ARIMA

  -  The X-11, X-12-ARIMA and X-13-ARIMA methods are based on Census II decomposition.
  -  These allow adjustments for trading days and other explanatory variables.
  -  Known outliers can be omitted.
  -  Level shifts and ramp effects can be modelled.
  -  Missing values estimated and replaced.
  -  Holiday factors (e.g., Easter, Labour Day) can be estimated.

## X-13ARIMA-SEATS

**Advantages**

  - Model-based
  - Smooth trend estimate
  - Allows estimates at end points
  - Allows changing seasonality
  - Developed for economic data

\pause

**Disadvantages**

  -  Only developed for quarterly and monthly data

# STL decomposition

## STL decomposition

\fontsize{13}{14}\sf

  -  STL: "Seasonal and Trend decomposition using Loess"
  -  Very versatile and robust.
  -  Unlike X-12-ARIMA, STL will handle any type of seasonality.
  -  Seasonal component allowed to change over time, and rate of change controlled by user.
  -  Smoothness of trend-cycle also controlled by user.
  -  Robust to outliers
  -  Not trading day or calendar adjustments.
  -  Only additive.
  -  Take logs to get multiplicative decomposition.
  -  Use Box-Cox transformations to get other decompositions.

## STL decomposition

```{r stlwindow9, warning=FALSE, fig.width=8.5, fig.height=3.4}
us_retail_employment %>%
  model(STL(Employed ~ season(window=9), robust=TRUE)) %>%
  components() %>% autoplot() +
    labs(title = "STL decomposition: US retail employment")
```

## STL decomposition

```{r stlwindowanim, echo=FALSE, warning=FALSE, message=FALSE, fig.show='animate', interval=1/10,  fig.height=5.35, fig.width=8, aniopts='controls,buttonsize=0.3cm,width=11.5cm', eval=TRUE}
s_windows <- seq(5,55,by=2)
stl_defs <- purrr::map(s_windows, function(s_window){
  STL(Employed ~ season(window=s_window), robust=TRUE)
})
names(stl_defs) <- sprintf("season(window=%02d)", s_windows)

us_retail_employment %>%
  model(!!!stl_defs) %>%
  components() %>%
  as_tibble() %>%
  pivot_longer(Employed:remainder,
               names_to = "component", names_ptypes = list(component = factor(levels = c("Employed", "trend", "season_year", "remainder"))),
               values_to = "Employed") %>%
  ggplot(aes(x = Month, y = Employed)) +
  geom_line() +
  facet_grid(rows = vars(component), scales = "free_y") +
  labs(title = "STL decomposition of US retail employment",
       subtitle = "{closest_state}") +
  transition_states(.model)
```

\vspace{10cm}

## STL decomposition

```{r echo = TRUE, results = 'hide'}
us_retail_employment %>%
  model(STL(Employed ~ season(window=5))) %>%
  components()

us_retail_employment %>%
  model(STL(Employed ~ trend(window=15) +
                       season(window="periodic"),
            robust = TRUE)
  ) %>% components()
```

\fontsize{12}{13}\sf

  -  `trend(window = ?)` controls wiggliness of trend component.
  -  `season(window = ?)` controls variation on seasonal component.
  -  `season(window = 'periodic')` is equivalent to an infinite window.

## STL decomposition

```{r mstl, fig.width=8.5, fig.height=3.4}
us_retail_employment %>%
  model(STL(Employed)) %>%
  components() %>% autoplot()
```

\only<2>{\begin{textblock}{7}(8,0.2)\fontsize{11}{11}\sf
\begin{alertblock}{}
\begin{itemize}\tightlist
\item \texttt{STL()} chooses \texttt{season(window=13)} by default
\item Can include transformations.
\end{itemize}
\end{alertblock}
\end{textblock}}

## STL decomposition
\fontsize{13}{14.5}\sf

- Algorithm that updates trend and seasonal components iteratively.
- Starts with $\hat{T}_t=0$
- Uses a mixture of loess and moving averages to successively refine the trend and seasonal estimates.
- The trend window controls loess bandwidth applied to deasonalised values.
- The season window controls loess bandwidth applied to detrended subseries.
- Robustness weights based on remainder.
- Default season `window = 13`
- Default trend `window = nextodd(` \newline\mbox{}\hfill `ceiling((1.5*period)/(1-(1.5/s.window)))`
