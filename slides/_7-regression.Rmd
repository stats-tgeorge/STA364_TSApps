---
title: "STA 364 Time Series Applications"
author: "Ch7. Regression models"
date: "OTexts.org/fpp3/"
classoption: aspectratio=169
titlecolor: cornellpurple
toc: true
output:
  binb::monash:
    colortheme: monashwhite
    fig_width: 7.5
    fig_height: 3
    keep_tex: no
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
source("setup.R")
```

# The linear model with time series

## Simple regression and forecasting

\begin{block}{}\vspace*{-0.3cm}
\[
  y_t = \beta_0 + \beta_1 x_{1,t} + \varepsilon_t.
\]
\end{block}

* $y_t$ is the variable we want to predict: the "response" variable
* Each $x_{t}$ is numerical and is called a "predictor".
 They are usually assumed to be known for all past and future times.
* The coefficients $\beta_1$ is the change in $y_t$ for a unit increase in $x_t$.
* $\beta_0$ is the intercept of the line (when $x=0$).
* $\varepsilon_t$ is a white noise error term, and $\varepsilon\sim N(0,\sigma_{\varepsilon})$


## SLR Example: : US consumption expenditure
```{r singlePredictor, echo=F, fig.height=4}
us_change %>%
  pivot_longer(c(Consumption, Income), names_to="Series") %>%
  autoplot(value) +
  labs(y = "% change")

```

## SLR Example: : US consumption expenditure
\small
```{r fig.height=2, fig.width=4}
us_change %>%
  ggplot(aes(x = Income, y = Consumption)) +
  labs(y = "Consumption (quarterly % change)",
       x = "Income (quarterly % change)") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```


## Fit SLR Model
\small
```{r}
# Fit the simple linear time series regression model
us_change %>%
  model(TSLM(Consumption ~ Income)) %>%
  report()
```

* coefficients are our estimates for our $\beta$'s

## Multiple regression and forecasting

\begin{block}{}\vspace*{-0.3cm}
\[
  y_t = \beta_0 + \beta_1 x_{1,t} + \beta_2x_{2,t} + ... + \beta_kx_{k,t} + \varepsilon_t.
\]
\end{block}

* $y_t$ is the variable we want to predict: the "response" variable
* Each $x_{j,t}$ is numerical and is called a "predictor".
 They are usually assumed to be known for all past and future times.
* The coefficients $\beta_1,\dots,\beta_k$ measure the effect of each
predictor after taking account of the effect of all other predictors
in the model.

That is, the coefficients measure the **marginal effects**.

* $\varepsilon_t$ is a white noise error term


## Example: US consumption expenditure

```{r MultiPredictors, echo=FALSE, fig.height=4}
us_change %>%
  gather("Measure", "Change", Consumption, Income, Production, Savings, Unemployment) %>%
  ggplot(aes(x = Quarter, y = Change, colour = Measure)) +
  geom_line() +
  facet_grid(vars(Measure), scales = "free_y") +
  labs(y="") +
  guides(colour="none")
```

## Example: US consumption expenditure

```{r ScatterMatrix, echo=FALSE, fig.height=4, fig.width=8}
us_change %>%
  as_tibble() %>%
  select(-Quarter) %>%
  GGally::ggpairs()
```

## Example: US consumption expenditure

\fontsize{7}{7}\sf

```{r usestim, echo=TRUE}
fit_consMR <- us_change %>%
  model(lm = TSLM(Consumption ~ Income + Production + Unemployment + Savings))
report(fit_consMR)
```

## Example: US consumption expenditure

```{r usfitted1, echo=FALSE}
augment(fit_consMR) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = NULL, title = "Percent change in US consumption expenditure") +
  scale_colour_manual(values=c(Data="black",Fitted="#D55E00")) +
  guides(colour = guide_legend(title = NULL))
```

## Example: US consumption expenditure

```{r usfitted2, echo=FALSE, message=FALSE, warning=FALSE}
augment(fit_consMR) %>%
  ggplot(aes(y=.fitted, x=Consumption)) +
    geom_point() +
    labs(y="Fitted (predicted values)",
         x="Data (actual values)",
         title ="Percentage change in US consumption expenditure") +
    geom_abline(intercept=0, slope=1)
```

## Example: US consumption expenditure

```{r, echo=TRUE}
fit_consMR %>% gg_tsresiduals()
```

# Some useful predictors for linear models

## Trend

**Linear trend**
\[
  x_t = t
\]

* $t=1,2,\dots,T$
* Strong assumption that trend will continue.

## Nonlinear trend

**Piecewise linear trend with bend at $\tau$**
\begin{align*}
x_{1,t} &= t \\
x_{2,t} &= \left\{ \begin{array}{ll}
  0 & t <\tau\\
  (t-\tau) & t \ge \tau
\end{array}\right.
\end{align*}
\pause\vspace*{1cm}

**Quadratic or higher order trend**
\[
  x_{1,t} =t,\quad x_{2,t}=t^2,\quad \dots
\]
\pause\vspace*{-0.5cm}

\centerline{\textcolor{orange}{\textbf{NOT RECOMMENDED!}}}

## Dummy variables

\begin{textblock}{6}(0.4,1.5)
If a categorical variable takes only two values (e.g., `Yes'
or `No'), then an equivalent numerical variable can be constructed
taking value 1 if yes and 0 if no. This is called a \textbf{dummy variable}.
\end{textblock}

\placefig{7.7}{1.5}{width=5cm}{dummy2}

## Dummy variables

\begin{textblock}{6}(0.4,1.5)
If there are more than two categories, then the variable can
be coded using several dummy variables (one fewer than the total
number of categories).

\end{textblock}

\placefig{7.7}{1.5}{width=7.3cm}{dummy3}

## Beware of the dummy variable trap!
* Using one dummy for each category gives too many dummy variables!

* The regression will then be singular and inestimable.

* Either omit the constant, or omit the dummy for one category.

* The coefficients of the dummies are relative to the omitted category.

## Uses of dummy variables
\fontsize{13}{15}\sf

**Seasonal dummies**

* For quarterly data: use 3 dummies
* For monthly data: use 11 dummies
* For daily data: use 6 dummies
* What to do with weekly data?

\pause

**Outliers**

* If there is an outlier, you can use a dummy variable to remove its effect.

\pause

**Public holidays**

* For daily data: if it is a public holiday, dummy=1, otherwise dummy=0.

## Beer production revisited

```{r, echo=FALSE}
recent_production <- aus_production %>% filter(year(Quarter) >= 1992)
recent_production %>% autoplot(Beer) +
  labs(y="Megalitres",title ="Australian quarterly beer production")
```

\pause

\begin{block}{Regression model}
\centering
$y_t = \beta_0 + \beta_1 t + \beta_2d_{2,t} + \beta_3 d_{3,t} + \beta_4 d_{4,t} + \varepsilon_t$
\end{block}\vspace*{-0.3cm}

* $d_{i,t} = 1$ if $t$ is quarter $i$ and 0 otherwise.

## Beer production revisited
\fontsize{8}{8}\sf

```{r, echo=TRUE}
fit_beer <- recent_production %>% model(TSLM(Beer ~ trend() + season()))
report(fit_beer)
```

## Beer production revisited

```{r, fig.height=2.4}
augment(fit_beer) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Beer, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y="Megalitres",title ="Australian quarterly beer production") +
  scale_colour_manual(values = c(Data = "black", Fitted = "#D55E00"))
```

## Beer production revisited

```{r, fig.height=2.4}
augment(fit_beer) %>%
  ggplot(aes(x=Beer, y=.fitted, colour=factor(quarter(Quarter)))) +
    geom_point() +
    labs(y="Fitted", x="Actual values", title = "Quarterly beer production") +
    scale_colour_brewer(palette="Dark2", name="Quarter") +
    geom_abline(intercept=0, slope=1)
```

## Beer production revisited

```{r, echo=TRUE}
fit_beer %>% gg_tsresiduals()
```

## Beer production revisited

```{r, echo=TRUE}
fit_beer %>% forecast %>% autoplot(recent_production)
```

## Fourier series

Periodic seasonality can be handled using pairs of Fourier terms:
$$
s_{k}(t) = \sin\left(\frac{2\pi k t}{m}\right)\qquad c_{k}(t) = \cos\left(\frac{2\pi k t}{m}\right)
$$
$$
y_t = a + bt + \sum_{k=1}^K \left[\alpha_k s_k(t) + \beta_k c_k(t)\right] + \varepsilon_t$$

* Every periodic function can be approximated by sums of sin and cos terms for large enough $K$.
* Choose $K$ by minimizing AICc.
* Called "harmonic regression"

```r
TSLM(y ~ trend() + fourier(K))
```


## Harmonic regression: beer production

\fontsize{9}{9}\sf

```{r fourierbeer, echo=TRUE}
fourier_beer <- recent_production %>% model(TSLM(Beer ~ trend() + fourier(K=2)))
report(fourier_beer)
```

## Harmonic regression: eating-out expenditure

```{r cafe, echo=TRUE, fig.height=2}
aus_cafe <- aus_retail %>% filter(
    Industry == "Cafes, restaurants and takeaway food services",
    year(Month) %in% 2004:2018
  ) %>% summarise(Turnover = sum(Turnover))
aus_cafe %>% autoplot(Turnover)
```

## Harmonic regression: eating-out expenditure

\small

```{r cafefit, dependson='cafe', fig.height=5, echo=TRUE}
fit <- aus_cafe %>%
  model(K1 = TSLM(log(Turnover) ~ trend() + fourier(K = 1)),
        K2 = TSLM(log(Turnover) ~ trend() + fourier(K = 2)),
        K3 = TSLM(log(Turnover) ~ trend() + fourier(K = 3)),
        K4 = TSLM(log(Turnover) ~ trend() + fourier(K = 4)),
        K5 = TSLM(log(Turnover) ~ trend() + fourier(K = 5)),
        K6 = TSLM(log(Turnover) ~ trend() + fourier(K = 6))) 
accuracy(fit) %>% 
        select(.model, .type, RMSE, MAE, MAPE, MASE)
```

## Harmonic regression: eating-out expenditure

```{r, echo=FALSE}
cafe_plot <- function(...){
  fit %>%
    select(...) %>%
    forecast() %>% autoplot(aus_cafe) +
    labs(title = sprintf("Log transformed %s, trend() + fourier(K = %s)", model_sum(select(fit,...)[[1]][[1]]), deparse(..1))) +
    geom_label(
      aes(x = yearmonth("2007 Jan"), y = 4250, label = paste0("RMSE = ", format(RMSE))),
      data = accuracy(select(fit,...))
    ) +
    geom_line(aes(y = .fitted), colour = "red", augment(select(fit, ...))) +
    ylim(c(1500, 5100))
}
```

```{r cafe1, dependson='cafe', fig.height=5, echo=FALSE}
cafe_plot(K = 1)
```

## Harmonic regression: eating-out expenditure

```{r cafe2, dependson='cafe', fig.height=5, echo=FALSE}
cafe_plot(K = 2)
```

## Harmonic regression: eating-out expenditure

```{r cafe3, dependson='cafe', fig.height=5, echo=FALSE}
cafe_plot(K = 3)
```

## Harmonic regression: eating-out expenditure

```{r cafe4, dependson='cafe', fig.height=5, echo=FALSE}
cafe_plot(K = 4)
```

## Harmonic regression: eating-out expenditure

```{r cafe5, dependson='cafe', fig.height=5, echo=FALSE}
cafe_plot(K = 5)
```

## Harmonic regression: eating-out expenditure

```{r cafe6, dependson='cafe', fig.height=5, echo=FALSE}
cafe_plot(K = 6)
```

## Intervention variables

**Spikes**

* Equivalent to a dummy variable for handling an outlier.
\pause

**Steps**

* Variable takes value 0 before the intervention and 1 afterwards.
\pause

**Change of slope**

* Variables take values 0 before the intervention and values $\{1,2,3,\dots\}$ afterwards.

## Holidays

**For monthly data**

* Christmas: always in December so part of monthly seasonal effect
* Easter: use a dummy variable $v_t=1$ if any part of Easter is in that month, $v_t=0$ otherwise.
* Ramadan and Chinese new year similar.


## Distributed lags

Lagged values of a predictor.

Example: $x$ is advertising which has a delayed effect

\begin{align*}
  x_{1} &= \text{advertising for previous month;} \\
  x_{2} &= \text{advertising for two months previously;} \\
        & \vdots \\
  x_{m} &= \text{advertising for $m$ months previously.}
\end{align*}

## Example: Boston marathon winning times

```{r, fig.height=2.6, echo=TRUE}
marathon <- boston_marathon %>%
  filter(Event == "Men's open division") %>%
  select(-Event) %>%
  mutate(Minutes = as.numeric(Time)/60)
marathon %>% autoplot(Minutes) + labs(y="Winning times in minutes")
```

## Example: Boston marathon winning times

\fontsize{9}{9}\sf

```{r, echo=TRUE}
fit_trends <- marathon %>%
  model(
    # Linear trend
    linear = TSLM(Minutes ~ trend()),
    # Exponential trend
    exponential = TSLM(log(Minutes) ~ trend()),
    # Piecewise linear trend
    piecewise = TSLM(Minutes ~ trend(knots = c(1940, 1980)))
  )
```

```{r}
fit_trends
```

## Example: Boston marathon winning times

```{r, echo=TRUE, eval=FALSE}
fit_trends %>% forecast(h=10) %>% autoplot(marathon)
```

```{r, echo=FALSE, message=TRUE, warning=FALSE, fig.height=3.3}
fc_trends <- fit_trends %>% forecast(h = 10)
marathon %>%
  autoplot(Minutes) +
  geom_line(data = fitted(fit_trends),
            aes(y = .fitted, colour = .model)) +
  autolayer(fc_trends, alpha = 0.5, level = 95) +
  labs(y = "Minutes",
       title = "Boston marathon winning times")
```

## Example: Boston marathon winning times

```{r residPiecewise, message=FALSE, warning=FALSE}
fit_trends %>%
  select(piecewise) %>%
  gg_tsresiduals()
```

# Residual diagnostics

## Multiple regression and forecasting
For forecasting purposes, we require the following assumptions:

* $\varepsilon_t$ are uncorrelated and zero mean

* $\varepsilon_t$ are uncorrelated with each $x_{j,t}$.
\pause

It is **useful** to also have $\varepsilon_t \sim \text{N}(0,\sigma^2)$ when producing prediction intervals or doing statistical tests.

## Residual plots

Useful for spotting outliers and whether the linear model was
appropriate.

* Scatterplot of residuals $\varepsilon_t$ against each predictor $x_{j,t}$.

* Scatterplot residuals against the fitted values $\hat y_t$

* Expect to see scatterplots resembling a horizontal band with
no values too far from the band and no patterns such as curvature or
increasing spread.

## Residual patterns

* If a plot of the residuals vs any predictor in the model shows a pattern, then the relationship is nonlinear.

* If a plot of the residuals vs any predictor **not** in the model shows a pattern, then the predictor should be added to the model.

* If a plot of the residuals vs fitted values shows a pattern, then there is heteroscedasticity in the errors. (Could try a transformation.)

# Forecasting with regression

## Ex-ante versus ex-post forecasts

 * *Ex ante forecasts* are made using only information available in advance.
    - require forecasts of predictors
 * *Ex post forecasts* are made using later information on the predictors.
    - useful for studying behaviour of forecasting models.

 * trend, seasonal and calendar variables are all known in advance, so these don't need to be forecast.

## Scenario based forecasting

 * Assumes possible scenarios for the predictor variables
 * Prediction intervals for scenario based forecasts do not include the uncertainty associated with the future values of the predictor variables.

## Building a predictive regression model {-}

 * If getting forecasts of predictors is difficult, you can use lagged predictors instead.
$$y_{t}=\beta_0+\beta_1x_{1,t-h}+\dots+\beta_kx_{k,t-h}+\varepsilon_{t}$$

 * A different model for each forecast horizon $h$.

## US Consumption

```{r usconsumptionf, echo=TRUE}
fit_consBest <- us_change %>%
  model(
    TSLM(Consumption ~ Income + Savings + Unemployment)
  )

future_scenarios <- scenarios(
  Increase = new_data(us_change, 4) %>%
    mutate(Income=1, Savings=0.5, Unemployment=0),
  Decrease = new_data(us_change, 4) %>%
    mutate(Income=-1, Savings=-0.5, Unemployment=0),
  names_to = "Scenario")

fc <- forecast(fit_consBest, new_data = future_scenarios)
```

## US Consumption

```{r usconsumptionf2, echo=TRUE}
us_change %>% autoplot(Consumption) +
  labs(y="% change in US consumption") +
  autolayer(fc) +
  labs(title = "US consumption", y = "% change")
```


# Correlation, causation and forecasting

## Correlation is not causation
* When $x$ is useful for predicting $y$, it is not necessarily causing $y$.

* e.g., predict number of drownings $y$ using number of ice-creams sold $x$.

* Correlations are useful for forecasting, even when there is no causality.

* Better models usually involve causal relationships (e.g., temperature $x$ and people $z$ to predict drownings $y$).

## Multicollinearity
In regression analysis, multicollinearity occurs when:

*  Two  predictors are highly  correlated (i.e., the correlation between them is close to $\pm1$).
* A linear combination of some of the predictors is highly correlated  with another predictor.
*  A linear combination of one subset of predictors is highly correlated with a linear combination of another
  subset of predictors.

## Multicollinearity

If multicollinearity exists\dots

* the numerical estimates of coefficients may be wrong (worse in Excel than in a statistics package)
* don't rely on the $p$-values to determine significance.
* there is no problem with model *predictions* provided the predictors used for forecasting are within the range used for fitting.
* omitting variables can help.
* combining variables can help.
